{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "#from sklearn.covariance import EllipticEnvelope\n",
    "#from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a1ae9b250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outlier target values\n",
    "outlier = []\n",
    "for i in tweets['Is_Unreliable']:\n",
    "    if i == 0:\n",
    "        i = 1\n",
    "    else:\n",
    "        i = -1\n",
    "    outlier.append(i)\n",
    "tweets['outlier_target'] = outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>outlier_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus is spreading wild wide and cities ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This morning, Sunnybrook discharged home the p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This afternoon, @WHO declared #coronavirus a p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinese health authorities announced Sunday th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local communities band together to show their ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable Category  \\\n",
       "280              0      NaN   \n",
       "281              0      NaN   \n",
       "282              0      NaN   \n",
       "283              0      NaN   \n",
       "284              0      NaN   \n",
       "..             ...      ...   \n",
       "555              0      NaN   \n",
       "556              0      NaN   \n",
       "557              0      NaN   \n",
       "558              0      NaN   \n",
       "559              0      NaN   \n",
       "\n",
       "                                                 Tweet  outlier_target  \n",
       "280  Coronavirus is spreading wild wide and cities ...               1  \n",
       "281  This morning, Sunnybrook discharged home the p...               1  \n",
       "282  This afternoon, @WHO declared #coronavirus a p...               1  \n",
       "283  Chinese health authorities announced Sunday th...               1  \n",
       "284  Local communities band together to show their ...               1  \n",
       "..                                                 ...             ...  \n",
       "555  BREAKING: Harvard classes will move online sta...               1  \n",
       "556  Singularity University is hosting a FREE Virtu...               1  \n",
       "557  Coronavirus: how does it spread and what are t...               1  \n",
       "558  Stanford just cancelled classes for the rest o...               1  \n",
       "559  Tech conferences were cancelled in #Waterloo R...               1  \n",
       "\n",
       "[280 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reliable_tweets = tweets[tweets['outlier_target'] == 1]\n",
    "reliable_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15,\n",
    "                   lowercase = True,\n",
    "                   lemmatize = True,\n",
    "                   pmi = True,\n",
    "                   pmi_positive = True,\n",
    "                   spmi_k = 5,\n",
    "                   laplace_smoothing = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.fit_transform(reliable_tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yet</th>\n",
       "      <th>yokohama</th>\n",
       "      <th>york</th>\n",
       "      <th>yorku</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.942702</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.097432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.896590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00831</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097432</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.189955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows × 1164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              !         #        (        )         ,    -   --         .  \\\n",
       "!           0.0  0.000000  0.00000  0.00000  0.000000  0.0  0.0  0.000000   \n",
       "#           0.0  1.942702  0.00000  0.00000  0.097432  0.0  0.0  0.896590   \n",
       "(           0.0  0.000000  0.00000  0.00831  0.000000  0.0  0.0  0.000000   \n",
       ")           0.0  0.000000  0.00831  0.00000  0.000000  0.0  0.0  0.000000   \n",
       ",           0.0  0.097432  0.00000  0.00000  0.189955  0.0  0.0  0.196282   \n",
       "...         ...       ...      ...      ...       ...  ...  ...       ...   \n",
       "zone        0.0  0.000000  0.00000  0.00000  0.000000  0.0  0.0  0.000000   \n",
       "zuckerberg  0.0  0.000000  0.00000  0.00000  0.000000  0.0  0.0  0.000000   \n",
       "—           0.0  0.000000  0.00000  0.00000  0.000000  0.0  0.0  0.000000   \n",
       "‘           0.0  0.000000  0.00000  0.00000  0.000000  0.0  0.0  0.000000   \n",
       "’           0.0  0.000000  0.00000  0.00000  0.000000  0.0  0.0  0.000000   \n",
       "\n",
       "            ...    1  ...  yeah  yet  yokohama  york  yorku  zone  zuckerberg  \\\n",
       "!           0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       "#           0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       "(           0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       ")           0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       ",           0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       "...         ...  ...  ...   ...  ...       ...   ...    ...   ...         ...   \n",
       "zone        0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       "zuckerberg  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       "—           0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       "‘           0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       "’           0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0         0.0   \n",
       "\n",
       "              —    ‘    ’  \n",
       "!           0.0  0.0  0.0  \n",
       "#           0.0  0.0  0.0  \n",
       "(           0.0  0.0  0.0  \n",
       ")           0.0  0.0  0.0  \n",
       ",           0.0  0.0  0.0  \n",
       "...         ...  ...  ...  \n",
       "zone        0.0  0.0  0.0  \n",
       "zuckerberg  0.0  0.0  0.0  \n",
       "—           0.0  0.0  0.0  \n",
       "‘           0.0  0.0  0.0  \n",
       "’           0.0  0.0  0.0  \n",
       "\n",
       "[1164 rows x 1164 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164, 1164)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00114587,  0.00196066],\n",
       "       [-0.98132672,  0.01866829],\n",
       "       [ 0.00140299,  0.0021934 ],\n",
       "       ...,\n",
       "       [ 0.00114587,  0.00196066],\n",
       "       [ 0.00114587,  0.00196066],\n",
       "       [ 0.00114587,  0.00196066]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>-0.981327</td>\n",
       "      <td>0.018668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.068295</td>\n",
       "      <td>-0.131698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—</th>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comp 1    Comp 2\n",
       "!           0.001146  0.001961\n",
       "#          -0.981327  0.018668\n",
       "(           0.001403  0.002193\n",
       ")           0.001403  0.002193\n",
       ",          -0.068295 -0.131698\n",
       "...              ...       ...\n",
       "zone        0.001146  0.001961\n",
       "zuckerberg  0.001146  0.001961\n",
       "—           0.001146  0.001961\n",
       "‘           0.001146  0.001961\n",
       "’           0.001146  0.001961\n",
       "\n",
       "[1164 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00285074,  0.00351954, -0.00419862, ...,  0.00222934,\n",
       "        -0.0022456 ,  0.00298571],\n",
       "       [-0.85982016, -1.02727651,  0.59209196, ..., -0.29686581,\n",
       "         0.49005825, -0.70942855],\n",
       "       [ 0.72435495,  0.51433802, -0.3672281 , ...,  0.51165927,\n",
       "        -0.66475361,  0.20459676],\n",
       "       ...,\n",
       "       [ 0.00285074,  0.00351954, -0.00419862, ...,  0.00222934,\n",
       "        -0.0022456 ,  0.00298571],\n",
       "       [ 0.00285074,  0.00351954, -0.00419862, ...,  0.00222934,\n",
       "        -0.0022456 ,  0.00298571],\n",
       "       [ 0.00285074,  0.00351954, -0.00419862, ...,  0.00222934,\n",
       "        -0.0022456 ,  0.00298571]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['outlier_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24038557, -0.18707671,  0.26122477, ..., -0.30826215,\n",
       "         0.30118149, -0.26070103],\n",
       "       [-0.18524225, -0.17389285,  0.24056809, ..., -0.23426752,\n",
       "         0.21647577, -0.18693359],\n",
       "       [-0.24793991, -0.23303032,  0.322157  , ..., -0.31309981,\n",
       "         0.28938289, -0.25024002],\n",
       "       ...,\n",
       "       [-0.21093231, -0.17270131,  0.23169931, ..., -0.26860127,\n",
       "         0.24801811, -0.21197979],\n",
       "       [-0.1922629 , -0.1694073 ,  0.21270488, ..., -0.18467704,\n",
       "         0.1993586 , -0.21059528],\n",
       "       [-0.08919131, -0.12376765,  0.12558547, ..., -0.08490222,\n",
       "         0.0877523 , -0.09522431]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_fraction = 0.05\n",
    "Nsamples_unreliable = int(np.round(280/(1-resample_fraction)*resample_fraction))\n",
    "\n",
    "resample_dict = {\n",
    "    -1: Nsamples_unreliable,\n",
    "    1: 280\n",
    "}\n",
    "\n",
    "underSample = RandomUnderSampler(sampling_strategy = resample_dict,\n",
    "                                 random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imb, y_imb = underSample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', OneClassSVM(nu = resample_fraction))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv, scoring = 'f1_macro')\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X_imb,\n",
    "                        y = y_imb,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1_macro', 'precision_macro', 'recall_macro'],\n",
    "                        return_estimator = True,\n",
    "                        return_train_score = True)\n",
    "\n",
    "test_auc = scores['test_roc_auc']\n",
    "train_auc = scores['train_roc_auc']\n",
    "\n",
    "test_accuracy = scores['test_accuracy']\n",
    "train_accuracy = scores['train_accuracy']\n",
    "\n",
    "test_f1 = scores['test_f1_macro']\n",
    "train_f1 = scores['train_f1_macro']\n",
    "\n",
    "test_precision = scores['test_precision_macro']\n",
    "train_precision = scores['train_precision_macro']\n",
    "\n",
    "test_recall = scores['test_recall_macro']\n",
    "train_recall = scores['train_recall_macro']\n",
    "\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.581547619047619"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9152542372881356"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5759382063364364"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5818351434140907"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5767857142857142"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
