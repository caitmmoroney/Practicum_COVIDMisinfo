{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "#from sklearn.covariance import EllipticEnvelope\n",
    "#from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x10698fe50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outlier target values\n",
    "outlier = []\n",
    "for i in tweets['Is_Unreliable']:\n",
    "    if i == 0:\n",
    "        i = 1\n",
    "    else:\n",
    "        i = -1\n",
    "    outlier.append(i)\n",
    "tweets['outlier_target'] = outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>outlier_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus is spreading wild wide and cities ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This morning, Sunnybrook discharged home the p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This afternoon, @WHO declared #coronavirus a p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinese health authorities announced Sunday th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local communities band together to show their ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable Category  \\\n",
       "280              0      NaN   \n",
       "281              0      NaN   \n",
       "282              0      NaN   \n",
       "283              0      NaN   \n",
       "284              0      NaN   \n",
       "..             ...      ...   \n",
       "555              0      NaN   \n",
       "556              0      NaN   \n",
       "557              0      NaN   \n",
       "558              0      NaN   \n",
       "559              0      NaN   \n",
       "\n",
       "                                                 Tweet  outlier_target  \n",
       "280  Coronavirus is spreading wild wide and cities ...               1  \n",
       "281  This morning, Sunnybrook discharged home the p...               1  \n",
       "282  This afternoon, @WHO declared #coronavirus a p...               1  \n",
       "283  Chinese health authorities announced Sunday th...               1  \n",
       "284  Local communities band together to show their ...               1  \n",
       "..                                                 ...             ...  \n",
       "555  BREAKING: Harvard classes will move online sta...               1  \n",
       "556  Singularity University is hosting a FREE Virtu...               1  \n",
       "557  Coronavirus: how does it spread and what are t...               1  \n",
       "558  Stanford just cancelled classes for the rest o...               1  \n",
       "559  Tech conferences were cancelled in #Waterloo R...               1  \n",
       "\n",
       "[280 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reliable_tweets = tweets[tweets['outlier_target'] == 1]\n",
    "reliable_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15, lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.fit_transform(reliable_tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yet</th>\n",
       "      <th>yokohama</th>\n",
       "      <th>york</th>\n",
       "      <th>yorku</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>2</td>\n",
       "      <td>392</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows × 1164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             !    #   (   )   ,   -  --    .  ...  1  ...  yeah  yet  \\\n",
       "!           18    2   0   1   2   2   0    5    3  0  ...     0    0   \n",
       "#            2  392   8   8  49  10   3  119    0  1  ...     0    0   \n",
       "(            0    8   2  23   9   0   0    6    0  0  ...     0    0   \n",
       ")            1    8  23   2   9   0   0    6    0  0  ...     0    0   \n",
       ",            2   49   9   9  46   4   3   48    1  2  ...     1    0   \n",
       "...         ..  ...  ..  ..  ..  ..  ..  ...  ... ..  ...   ...  ...   \n",
       "zone         0    1   0   0   1   0   0    1    0  0  ...     0    0   \n",
       "zuckerberg   0    0   0   0   0   0   0    0    0  0  ...     0    0   \n",
       "—            0    1   0   0   1   0   0    1    0  0  ...     0    0   \n",
       "‘            0    0   0   0   1   0   0    0    0  0  ...     0    0   \n",
       "’            6   19   1   1   9   2   0   12    2  0  ...     2    0   \n",
       "\n",
       "            yokohama  york  yorku  zone  zuckerberg  —  ‘   ’  \n",
       "!                  0     0      0     0           0  0  0   6  \n",
       "#                  0     8      2     1           0  1  0  19  \n",
       "(                  0     0      0     0           0  0  0   1  \n",
       ")                  0     0      0     0           0  0  0   1  \n",
       ",                  1     3      2     1           0  1  1   9  \n",
       "...              ...   ...    ...   ...         ... .. ..  ..  \n",
       "zone               0     0      0     0           0  0  0   0  \n",
       "zuckerberg         0     0      0     0           0  0  0   0  \n",
       "—                  0     0      0     0           0  0  0   0  \n",
       "‘                  0     0      0     0           0  0  0   1  \n",
       "’                  0     0      0     0           0  0  1   4  \n",
       "\n",
       "[1164 rows x 1164 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164, 1164)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.09349859e-02,  8.57324220e-03],\n",
       "       [ 7.43548735e-01,  5.42420023e-01],\n",
       "       [-3.62339802e-02,  2.20327619e-02],\n",
       "       ...,\n",
       "       [ 3.99573690e-03, -3.13994639e-03],\n",
       "       [-1.90439816e-03, -4.98278016e-03],\n",
       "       [-2.97527689e-04,  3.71691480e-02]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.020935</td>\n",
       "      <td>0.008573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.743549</td>\n",
       "      <td>0.542420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>-0.036234</td>\n",
       "      <td>0.022033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>-0.037973</td>\n",
       "      <td>0.022526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.244438</td>\n",
       "      <td>0.169875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.002688</td>\n",
       "      <td>-0.004322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.005223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—</th>\n",
       "      <td>0.003996</td>\n",
       "      <td>-0.003140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>-0.001904</td>\n",
       "      <td>-0.004983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>-0.000298</td>\n",
       "      <td>0.037169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comp 1    Comp 2\n",
       "!          -0.020935  0.008573\n",
       "#           0.743549  0.542420\n",
       "(          -0.036234  0.022033\n",
       ")          -0.037973  0.022526\n",
       ",          -0.244438  0.169875\n",
       "...              ...       ...\n",
       "zone        0.002688 -0.004322\n",
       "zuckerberg  0.000004 -0.005223\n",
       "—           0.003996 -0.003140\n",
       "‘          -0.001904 -0.004983\n",
       "’          -0.000298  0.037169\n",
       "\n",
       "[1164 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXyU1fn//9eZ7AtJwIQ9kLDJlk0iIDtEhRIEUZRdKFY/VEF/alH82QIf2tpWcAFLVUAWlUrEDbAofAggIDsaUpYgIAHCmoUEyD7J9f0jYUxCgAGywOR6Ph55MPc9Z+65zoBvT87c932MiKCUUurOZ6nuApRSSlUMDXSllHIQGuhKKeUgNNCVUspBaKArpZSDcK6uN/b395egoKDqenullLoj7d69O0VEAsp7rtoCPSgoiF27dlXX2yul1B3JGHPsas/plItSSjmIahuhK6WUgldffZW+ffuSnp5OQkICkydPvulj6QhdKaWq0fbt2+nUqRPff/893bt3v6Vj6QhdKaWqwaRJk1i9ejVHjx7lvvvu48iRI8TGxjJkyBCmTJlyU8c01XUvl8jISNEvRZVSNdmOHTv4+OOPeeutt+jVqxc//PDDdV9jjNktIpHlPacjdKWUqirxn0HsdMhIAt/G/JR2H+HhnUlISKBt27a3fHgNdKWUqgrxn8HK5yA/m7gzBYx9/wBJF/fjX3cVWQVOiAjh4eFs3boVDw+Pm3oL/VJUKaWqQux0yM8GILy+E3HjvWlVx8L+CX706dOH1atXExcXd9NhDjpCV0qpqpGRVGozObOQ2h4Gy8WTJCTUqpApFx2hK6VUVfBtXGozwMvCf0Z4gm9jtm3bViFvoYGulFJVIWoKuJSZTnHxKNpfQTTQlVKqKoQ+Dg/NBt9AwBT9+dDsov0VROfQlVKqqoQ+XqEBXpaO0JVSykFooCullIPQQFdKKQehga6UUg5CA10ppRyEBrpSSjkIDXSllHIQdgW6MaafMeagMeawMeaK9ZGMMWONMcnGmLjin99VfKlKKaWu5boXFhljnIA5wANAErDTGLNCRPaXaRojIhMqoUallFJ2sGeE3hE4LCK/iEgesBQYVLllKaWUulH2BHoj4ESJ7aTifWU9aoyJN8Z8bowJLO9AxpinjTG7jDG7kpOTb6JcpZRSV2NPoJty9pVdiHQlECQiocBaYHF5BxKRuSISKSKRAQEBN1apUkqpa7In0JOAkiPuxsCpkg1EJFVEcos35wEdKqY8pZRS9rIn0HcCLY0xwcYYV2AYsKJkA2NMgxKbA4EDFVeiUkope1z3LBcRsRpjJgCrASdggYjsM8ZMB3aJyArgOWPMQMAKpAFjK7FmpZRS5TAiZafDq0ZkZKTs2rWrWt5bKaXuVMaY3SISWd5zeqWoUko5CA10pZRyEBroSinlIDTQlVLKQWigK6WUg9BAV0opB6GBrpRSDkIDXSmlHIQGulJKOQgNdKWUchAa6Eop5SA00JVSykFooCullIOoEYH+zTffEBERQVhYGF5eXtSrV48GDRrg4eGBn58fTk5O1K9fH4vFwl//+lfb65ycnBg+fLjtcXh4OH5+fri6utKoUSPCwsKIjY0FYPDgwYSHh9OiRQt8fX0JDw8nPDycLVu2VEuflVI1j8MGel5eHpmZmeTn5/P0008TExPDnj176NChA3PmzOH06dNkZ2eTnp6Oh4cH48ePp3Hjxly4cAGAAwcOICJs3LiRzMxMPDw8iIuL4+GHH+a9996jTZs2hIWFMX78eDIzM4mJiSEuLo758+fTvXt34uLiiIuLo0uXLpw/f76aPw2lVE3gcIF+4MABXnrpJe6++26+++47nn/+ec6ePUtqaioAFouFoKCgcl87btw4YmJiSEtL49///jfOzs48+OCDrFhRaoEmfH19mTt3Lt988w0nT57k559/5u677+all17i2LFjVxx34sSJ9O7dmyVLlpCTk1PhfVZKKXCQQM/MzGThwoV069aNcePGkZaWRr169Xjrrbe45557GDVqFIMGDWL48OGcPXuWESNG2KZEJk2aZDuOt7c348aNY9asWcTExODs7Mzw4cP59NNPr3jPZs2akZubS79+/YiIiCA+Pp42bdowY8YMtmzZwsKFC8nMzATgk08+YebMmWzZsoV27doxceJE9uzZU2Wfj1KqhhCRavnp0KGD3JI9MSJvtROZ6iu13CwS2CRQGv7uPXFy85TW4ffKgQMHSjWPj4+Xt956S7y9vSU6OrrUc15eXjJ16lSZMWOGnD9/XurXry+dOnUSLy8vsVqt0qhRI/H09BQRkTFjxkhQUJAEBwcLIOvXry91rPXr10vPnj3lvvvuk1q1al1RdnZ2trzzzjvi6uoqb7755q19BkqpGoeipT/LzdU7c4Qe/xmsfA4yTgDC54+509H7LNlf/RH3Fp04cclw/28eYvr06bYpkJCQEF544QVCQ0NZv379VQ/t5+dHYGAg8fHxZGVl0bx5cy5cuIDVarW1mTFjBmvWrMHT05OXXnrJtv/YsWMsWrSIXbt2ERgYyOeff257zmq1smLFCoYPH868efOYPn06o0aNqvjPRilVY92ZgR47HfKzbZsPNnfm88fc+X6sN671W5B3IZmLeGK1WhkwYAAdOnQgMTERgEuXLlG/fv2rHrqwsJCTJ09y11134erqSmJiIsuXLy8V6BcuXOCZZ57hpZdeorCwkMWLF3P//fczaNAgvL296dKlCzExMTz44IMAvPXWW7Rq1YovvviCF154gb179/LKK69Qt27dyvl8lFI1knN1F3BTMpLK3d3OKx2fyEH4RA4i79RBnnrqMSZNmkTfvn2JioqiVq1aJCYm4ufnR3h4OAD+/v6ljrFx40YCAwPp0qULb7/9NgA9evSgsLCQdu3acfLkSdavX8/EiRN58cUXCQsL46233uLtt9+mY8eObNiwgZkzZ5Y6ZmhoKHFxcfj4+FTCh6GUUkVM0ZRM1YuMjJRdu3bd3Ivfbl883VJaUqE/3fJmA9DIz4MfJve5lRKVUuq2Y4zZLSKR5T13Z065RE0BF49Su7LElTesjwPg4eLEpL53V0dlSilVbe7MKZfQouAmdjpkJJHlUZ838oeyMrcjjfw8mNT3bh6OaFS9NSqlVBW7M6dclFKqhnK8KRellFJX0EBXSikHYVegG2P6GWMOGmMOG2MmX6PdEGOMGGPK/XVAKaVU5bluoBtjnIA5wG+AtsBwY0zbctrVAp4Dtld0kUoppa7PnhF6R+CwiPwiInnAUmBQOe3+DLwB6O0ElVKqGtgT6I2AklfxJBXvszHGRACBIvLNtQ5kjHnaGLPLGLMrOTn5hotVSil1dfYEuilnn+1cR2OMBXgbeKmcdqVfJDJXRCJFJDIgIMD+KpVSSl2XPYGeBASW2G4MnCqxXQtoD2wwxiQCnYEV+sWoUkpVLXsCfSfQ0hgTbIxxBYYBtiV8RCRDRPxFJEhEgoBtwEAR0auGlFKqCl030EXECkwAVgMHgM9EZJ8xZroxZmBlF6iUUso+dt3LRURWAavK7Jtylba9br0spZRSN0qvFHUwXbp0qe4SlFLVRAO9ipRc8agybdmy5Yp9BQUFVfLeSqnqpYF+Ez766CNCQ0MJCwtj9OjRHDt2jKioKEJDQ4mKiuL48eMAjB07lhdffJHevXvzyiuvkJaWxsMPP0xoaCidO3cmPj4egGnTpjFu3Dh69epFs2bNmD17tu29Hn74YTp06EC7du2YO3cuAO+99x4vv/yyrc2iRYuYOHEiAN7e3gBs2LCB3r17M2LECEJCQkhMTKR9+/a218ycOZNp06YBMHv2bNq2bUtoaCjDhg2rvA9OKVW5rrZ6dGX/dOjQoeKWwa5Ce/fulVatWklycrKIiKSmpsqAAQNk0aJFIiLy4YcfyqBBg0REZMyYMRIdHS1Wq1VERCZMmCDTpk0TEZHY2FgJCwsTEZGpU6fKfffdJzk5OZKcnCx16tSRvLw82/FFRLKysqRdu3aSkpIi586dk+bNm9tq6tevn2zatElERLy8vEREZP369eLp6Sm//PKLiIgcPXpU2rVrZ3vNjBkzZOrUqSIi0qBBA8nJyRERkfPnz1fkx6WUqmDALrlKruoI3V7xn8Hb7Vn3h0iGNEnD/9Q6AOrUqcPWrVsZMWIEAKNHj2bz5s22lz322GM4OTkBsHnzZkaPHg1Anz59SE1NJSMjA4Do6Gjc3Nzw9/enbt26nD17FigaPYeFhdG5c2dOnDjBoUOHCAgIoFmzZmzbto3U1FQOHjxI165dryi5Y8eOBAcHX7droaGhjBw5kk8++QRn5ztzzROllE652Cf+M1j5HGScQBBM7oWi7fjPym1uzK8X13p5edkeSzmLiVxu6+bmZtvn5OSE1Wplw4YNrF27lq1bt7Jnzx4iIiLIySm6Vc7QoUP57LPP+OKLLxg8eHCp9yzvvZ2dnSksLLRtXz4OwH/+8x+effZZdu/eTYcOHapsvl8pVbE00O0ROx3yswGICnbms31WUjMyIXY6aWlpdOnShaVLlwKwZMkSunXrVu5hevTowZIlS4CiOW5/f398fHyu+rYZGRnUrl0bT09PEhIS2LZtm+25Rx55hK+//ppPP/2UoUOHXrcL9erV49y5c6SmppKbm8s33xTddqewsJATJ07Qu3dv3njjDdLT07l06ZJ9n4tS6raiv1/bIyPJ9rBdXSde6+5Kz0VZOFkOELHnRWbPns24ceOYMWMGAQEBLFy4sNzDTJs2jd/+9reEhobi6enJ4sWLr/m2/fr14/333yc0NJS7776bzp07256rXbs2bdu2Zf/+/XTs2PG6XXBxcWHKlCl06tSJ4OBgWrduDRSdATNq1CgyMjIQEV544QX8/Pzs+VSUUrcZXVPUHm+3h4wTV+73DYQX9lZ9PUqpGkvXFL1VUVPAxaP0PhePov1KKXWb0EC3R+jj8NDsohE5pujPh2YX7VdKqduEzqHbK/RxDXCl1G1NR+hKKeUgNNCVUspBaKArpZSD0EBXSikHoYGulFIOQgNdKaUchAa6Uko5CA10pZRyEBroSinlIDTQlVLKQWigK6WUg9BAV0opB6GBrpRSDkIDXSmlHIRdgW6M6WeMOWiMOWyMmVzO8+ONMf81xsQZYzYbY9pWfKlKKaWu5bqBboxxAuYAvwHaAsPLCex/i0iIiIQDbwBvVXilSimlrsmeEXpH4LCI/CIiecBSYFDJBiJyocSmF1A9C5UqpVQNZs+KRY2AkiskJwGdyjYyxjwLvAi4An3KO5Ax5mngaYAmTZrcaK1KKaWuwZ5AN+Xsu2IELiJzgDnGmBHAH4Ex5bSZC8wFiIyM1FG8uiXx8fHExsaSkZGBr68vUVFRhIaGVndZSlUbewI9CQgssd0YOHWN9kuB926lKKWuJz4+npUrV5Kfnw9ARkYGK1euBNBQVzWWPXPoO4GWxphgY4wrMAxYUbKBMaZlic1o4FDFlajUlWJjY21hDrBkyRLS0tKIjY2txqqUql7XHaGLiNUYMwFYDTgBC0RknzFmOrBLRFYAE4wx9wP5wHnKmW5RqiJlZGSU2h45cmS5+5WqSeyZckFEVgGryuybUuLx8xVcl1LX5OvrW254+/r6VkM1St0e9EpRdUeKiorCxcWl1D4XFxeioqKqqSKlqp9dI3SlbjeXv/jUs1yU+pUGurpjhYaGaoArVYJOuSillIPQQFdKKQehga6UUg5CA10ppRyEBrpSSjkIDXSllHIQGuhKKeUgNNCVUspBaKArpZSD0EBXSikHoYGulFIOQgNdKaUchAa6Uko5CA10pZRyEBroSinlIDTQlVLKQWigK6WUg9BAV0opB6GBrpRSDkIDXSmlHIQGulJKOQgNdKWUchAa6Eop5SA00JVSykHYFejGmH7GmIPGmMPGmMnlPP+iMWa/MSbeGBNrjGla8aUqpZS6lusGujHGCZgD/AZoCww3xrQt0+wnIFJEQoHPgTcqulCllFLXZs8IvSNwWER+EZE8YCkwqGQDEVkvIlnFm9uAxhVbplJKqeuxJ9AbASdKbCcV77uaJ4Fvy3vCGPO0MWaXMWZXcnKy/VUqpZS6LnsC3ZSzT8ptaMwoIBKYUd7zIjJXRCJFJDIgIMD+KpVSSl2Xsx1tkoDAEtuNgVNlGxlj7gdeA3qKSG7FlKeUUspe9ozQdwItjTHBxhhXYBiwomQDY0wE8AEwUETOVXyZSimlrue6gS4iVmACsBo4AHwmIvuMMdONMQOLm80AvIFlxpg4Y8yKqxxOKaVUJbFnygURWQWsKrNvSonH91dwXUoppW6QXimqlFIOQgO9BpsyZQpr1669Yv+GDRsYMGBAua/55z//SYsWLTDGkJKSYtt//vx5Bg8eTGhoKB07dmTv3r2VVrdSqnx2TbnURD9vP8PW5Ue4lJaLdx037hvUnFad6ld3WRVq+vTpN/yarl27MmDAAHr16lVq/+uvv054eDhfffUVCQkJPPvss8TGxlZQpUope+gIvRw/bz/D+iUJXEorOvvyUlouA4f0Z9PKn6q5stI++ugjQkNDCQsLY/To0Rw7doyoqChCQ0OJiori+PHjZGRkEBQURGFhIQBZWVkEBgaSn5/P2LFj+fzzzwH47rvvaN26Nd26dePLL7+86ntGREQQFBR0xf79+/cTFRUFQOvWrUlMTOTs2bMV32ml1FVpoJdj6/IjWPMKbduFUsi59JMkbDxfjVWVtm/fPv7617+ybt069uzZw6xZs5gwYQJPPPEE8fHxjBw5kueeew5fX1/CwsL4/vvvAVi5ciV9+/bFxcXFdqycnByeeuopVq5cyaZNmzhz5swN1xMWFmb7H8GOHTs4duwYSUlJFdNZpZRdNNDLcXlkftmZ88cID+5O3sVqKqgc69atY8iQIfj7+wNQp04dtm7dyogRIwAYPXo0mzdvBmDo0KHExMQAsHTpUoYOHVrqWAkJCQQHB9OyZUuMMYwaNeqG65k8eTLnz58nPDycd999l4iICJyddUZPqaqk/8WVw7uOW6lQb1gnmEe7PIN3HbdqrApOn1nOL0dmkpN7miNHBOhwzfbGFN21YeDAgbz66qukpaWxe/du+vTpc9W2ZfXt25ezZ88SGRnJ/Pnzr/pePj4+LFy4EAARITg4mODgYDt7ppSqCDpCL8d9g5rj7Fr6o3F2tXDfoObVVFFRmCckvEZO7ilACAnN44svlrNv/8cApKWl0aVLF5YuXQrAkiVL6NatGwDe3t507NiR559/ngEDBuDk5FTq2K1bt+bo0aMcOXIEgE8//dT23OrVq4mLi7tmmAOkp6eTl5cHwPz58+nRowc+Pj4V0nellH000MvRqlN9eo9sbRuRe9dxo/fI1tV6lssvR2ZSWJht2w4KcmXECF8eGjCesLAwXnzxRWbPns3ChQsJDQ3l448/ZtasWbb2Q4cO5ZNPPrliugXA3d2duXPnEh0dTbdu3Wja9Orrk8yePZvGjRuTlJREaGgov/vd7wA4cOAA7dq1o3Xr1nz77bel3lspVTWMSLk3Tqx0kZGRsmvXrmp57ztR7LoWlH+TS0NUn8NVXY5SqpoYY3aLSGR5z+kI/Q7h7tbghvYrpWoeDfQ7RLPmf8Bi8Si1z2LxoFnzP1RTRUqp242e5XKHaFC/aNW/y2e5uLs1oFnzP9j2K6WUBvodpEH9QRrgSqmr0ikXpZRyEBroSinlIDTQlVLKQWigK6WUg9BAV0opB6GBrpRSDkIDXSmlHIQGulJKOQgNdKWUchAa6A6sf//+nDp1qrrLuONMmTKFtWvXXrF/w4YNDBgwACha5em+++7Dzc2NmTNnlmo3a9Ys2rdvT7t27XjnnXeqpGalQC/9dzglVzV67bUGGMtOQG8XcCOmT59+3TZ16tRh9uzZfP3116X27927l3nz5rFjxw5cXV3p168f0dHRtGzZsrLKVcpGR+gOpOyqRjm5p0hIeI3TZ5ZXd2lV6qOPPiI0NJSwsDBGjx7NsWPHiIqKIjQ0lKioKI4fP05GRgZBQUEUFhYtBp6VlUVgYCD5+fmMHTuWzz//HIDvvvuO1q1b061bN9si2AB169bl3nvvLbXYNhQt9NG5c2c8PT1xdnamZ8+efPXVV1XXeVWj2RXoxph+xpiDxpjDxpjJ5TzfwxjzozHGaowZUvFlKnuUXdUIoLAwmzn/nFQq4FauXEmnTp2IiIjg/vvv5+zZswB8//33hIeHEx4eTkREBBcvFq2KPWPGDO69915CQ0OZOnVqlffrRuzbt4+//vWvrFu3jj179jBr1iwmTJjAE088QXx8PCNHjuS5557D19eXsLAwvv/+ewBWrlxJ3759SwV0Tk4OTz31FCtXrmTTpk2cOXPmuu/fvn17Nm7cSGpqKllZWaxatYoTJ05UWn+VKum6Uy7GGCdgDvAAkATsNMasEJH9JZodB8YCenPuapSTe7rU9v//6mkee9yXxYtT+emnU/j7+5OWloYxhm3btmGMYf78+bzxxhu8+eabzJw5kzlz5tC1a1cuXbqEu7s7a9as4dChQ+zYsQMRYeDAgWzcuJEePXpUUy+v9PVPJ5mx+iCn0rMx+7/jnm598ff3B4qmRrZu3WobXY8ePZqXX34ZKFqWLyYmht69e7N06VKeeeaZUsdNSEggODjYNl0yatQo5s6de81a2rRpwyuvvMIDDzyAt7c3YWFhODvrzKaqGvaM0DsCh0XkFxHJA5ZSZlJWRBJFJB4orIQalZ3Krl70+t8akJiYT+/e9UoFXFJSEn379iUkJIQZM2awb98+ALp27WpbmzQ9PR1nZ2fWrFnDmjVriIiI4J577iEhIYFDhw5Ved+u5uufTvLql//lZHo2AqRn5bHhYDJf/3Tyqq8xxgAwcOBAvv32W9LS0ti9ezd9+vS5atsb8eSTT/Ljjz+yceNG6tSpo/PnqsrYE+iNgJK/MyYV77thxpinjTG7jDG7kpOTb+YQ6hpKrmpU72wOXban0fLwJRqfugjxn9naTZw4kQkTJvDf//6XDz74gJycHAAmT57M/Pnzyc7OpnPnziQkJCAivPrqq8TFxREXF8fhw4d58sknq6V/5Zmx+iDZ+QW2bfemYWTs38jrX+4AIC0tjS5durB06VIAlixZQrdu3QDw9vamY8eOPP/88wwYMAAnJ6dSx27dujVHjx7lyJEjAHz66ad21XTu3DkAjh8/zpdffsnw4cNvrZNK2cme3wXLG6Lc1MrSIjIXmAtFi0TfzDHU1V1e/OLC5tdocSgFp0K4P9iZwTEXeSlmAncBaY3vJyMjg0aNiv6fvHjxYtvrjxw5QkhICCEhIWzdupWEhAT69u3Ln/70J0aOHIm3tzcnT57ExcWFunXrVkcXr3AqvfR3Bq4BTfG9byhx7/9/hK2YRkREBLNnz2bcuHHMmDGDgIAAFi5caGs/dOhQHnvsMTZs2HDFsd3d3Zk7dy7R0dH4+/vTrVs39u7dC8CZM2eIjIzkwoULWCwW3nnnHfbv34+Pjw+PPvooqampuLi4MGfOHGrXrl2pn4FSlxmRa+eqMeY+YJqI9C3efhVARP5WTttFwDci8vn13jgyMlJ27dp1MzWr63m7PWT8+kvV4rg8ZmzJw8nZhYgHhzF48GBeeOEFGjVqROfOndm5cycbNmxg4sSJrF+/HicnJ9q2bcuiRYtwc3Nj1qxZzJ8/Hyga1X7yySc0b968unpXSte/r+NkmVAHaOTnwQ+Tr5xCUepOZ4zZLSKR5T5nR6A7Az8DUcBJYCcwQkT2ldN2ERro1W+aH+X/EmVgWnpVV1OpLs+hl5x28XBx4m+PhPBwxE3NDCp1W7tWoF93Dl1ErMAEYDVwAPhMRPYZY6YbYwYWv8G9xpgk4DHgA2PMFWGvqpBv4xvbfwd7OKIRf3skhEZ+HhiKRuYa5qqmuu4IvbLoCL0SxX8GK5+D/BJTES4e8NBsCH28+upSSt2yWxqhqztQ6ONF4e0bCJiiPzXMlXJ4esWDowp9XANcqRpGR+g1VGJiIm3atOGpp56iXbt2PPjgg2RnZ3PkyBH69etHhw4d6N69OwkJCRQUFNCsWTNEhPT0dCwWCxs3bgSge/fuHD58uJp7o5QCDfQa7dChQzz77LPs27cPPz8/vvjiC55++mneffdddu/ezcyZM3nmmWdwcnKiVatW7N+/n82bN9OhQwc2bdpEbm4uSUlJtGjRorq7opRCp1xqlJK31j2fVpsmTeoSHh4OQIcOHUhMTGTLli089thjttfk5uYCRSPxjRs3cvToUV599VXmzZtHz549uffee6ulL0qpK+kIvYYoe2vd3LyziKTZbq3r5OREWloafn5+tsv84+LiOHDgAFAU6Js2bWLHjh3079+f9PR0NmzYcFvdpEupmk4DvYYo79a6UMgvR35dbcfHx4fg4GCWLVsGgIiwZ88eEhMTGT9+PFu2bMFiseDu7k54eDgffPAB3bt3t7uGRYsWMWHCBAC+/vpr9u//9YadvXr1Qk9jVerWaKDXEGVvrXu1/UuWLOHDDz8kLCyMdu3asXx50QjeYrEQGBhI586dgaIR+8WLFwkJCbmpesoGulLq1ukceg3h7tageLqlSP36Lsz/MNB2y90//OHXW9l/9913pV6bmJhIQUEBrVu3Zvny5ezatYvly5fTqVMn+vfvT3JyMp6ensybN4/WrVuzcuVKnn/+eTw9Palfvz5LliyhXr16tuNt2bKFFStW8P333/OXv/yFL774AoBly5YxZMgQcnJyWLZs2Q2N/pVSOkKvMUreWvcyi8WDZs3tW5PE3jNiALp160aTJk1YtGgRw4YN44033ih1rC5dujBw4EBmzJhBXFyc7UZfeXl5JCYmsmDBAv73f/+3AnqtVM2iI/Qa4vKtdS+f5eLu1oBmzf9g21/WgU3r2bT0Iy6mppDr4kaj+vVZt24do0aNIiUlhbVr19qmXLy8vMjMzMTJyYns7GwWLFjA5s2b6dq1K8YYunfvzvTp01m4cCEXLlwgLy+Py7ec6NWrF126dCEuLo727dszbdo0RITExETi4uIYP348WVlZNG/enAULFuitaJW6Bh2h1yAN6g+ia9dNRPU5TIR+wscAABOPSURBVNeum64Z5mvm/pOLKckgQub5NDIz0nn/n/9k+/btPPPMM1y8eBEfHx+sViuxsbHk5OQQHR3NF198wcqVK2nTpg0//PAD3333Hfn5+UyYMIGpU6cyfPhwsrOzWbNmDVu3bgUgPT2d8PBwxo4dCxTN11utVp544gn+8Y9/EB8fT0hIiI7alboODXR1hU1LP8Kal1tqX15+Ps19PfHy8sLZ2ZnIyEh8fHwICAggPDwcEaFhw4YkJiaSkZGBq6sr8OsCGuvXr+fPf/4zn376KevWraOgoMC2UtLQoUOvqKGwsJD09HR69uwJwJgxY2xXpyqlyqeBXoNlZmYSHR1NWFgY7du3JyYmhqCgID5dt4lZazcza+1mUi5mApCdb2X/8SQ63RPB4sWLyc/Px8/Pj3PnzmGMoVGjRhw4cICUlBRyc3P56aef6NGjB7m5uRQWFvL73/+e+vXrIyJ4eXnh6upq+4L1/Pnz5daXk5PDgAEDqvIjUeqOpoFeg3333Xc0bNiQPXv2sHfvXvr16weAr48Pz9/fja4tgnhvwzZOpGXQsp4/5zIuMTM/H+8TJzgYF0dOaipt2rQhMDAQDw8PevbsiZeXFz/++CPR0dF88MEHrFu3jnPnzpGRkUFKSgqvv/46xhhSUlJ45513iIyMpHHjxmzYsIHIyKI7gnp5eXH8+HFq1apFWloaAB9//LFttK6UKp9+KVqDpKen8+8ZL/GM/3bISCIkN4A/rMrglVfqkJmZyfr16zl+/Di5WVm0ucuXdQcOcyk3D4Czaek0sFh4POEgZ61WLMCRpCQszs4UiCAiTJ06FWMMP/74I/v372fVqlUUFhbSsmVLMjIySEtLY/r06eTlFR1z586dAJw4cYIXXniBzMxMTp8+zaOPPgrAq6++yqRJkwgNDSU3N5eUlBQmTpxIs2bNquXzU+p2pyP0GiR96yf8a/5iyDhBek4ha+OT2D0WvC8lsnDhQgYPHoyzszNnU1L48IddpGVmUVBYyBe7/0tabh7H8/OpZbHgbgwGKBSht58fIoK3tzeenp7k5+ezdu1ajh49SoMGRee4b9u2jYsXL1JQUICbmxvvv/8+Xl5ezJkzB6vVSkJCAkFBQXTq1IkmTZowb948Jk+ezJ///Gfy8/Np3Lgxubm5DBgwQMNcqWuR4tFVVf906NBBVNUaGu4j7s5IWD2LPBXhIgGeRu5pYJH6tYwEBXlI164hAoizs7PU8fMTQDxcnKWhby0BxIA4g9zr4SFORYuWimvxn4A89thj4uTkJIDUqlVLPD09BZBp06ZJ06ZNbe0u/7i6usrIkSOlf//+4urqKhaLRRo3biyurq5ijJGgoCCxWCwCiKenp7Rs2VJERFasWCEdO3aU8PBwiYqKkjNnzly1zydPnpRHH330hj+rr776Svbt22fb/tOf/iT/93//d+MfulIVDNglV8lVDfQa5OjztaRdgEVkqo90C3SyhfTlgPX0NLbHxvy632JMqXYV9WOxWKRFixbi7u5uC3gXFxcxxogxRlxdXSUgIEAAcXNzE2OMNGnSRFatWiWFhYUiIjJv3jx58cUXS/UzPz//lj+rMWPGyLJly275OEpVtGsFuk651CS16tseujmDxUBoPQsNfQwAVuuv68teXmq2adP6FIpQGSvPFhYWcvjwYdtFRnl5eeTn59v+cfr4+JCVlQUU3cZXRAgODubRRx/Fx8cHi8XCCy+8wHvvvUfDhg155JFH8Pf3p3bt2owZM4aHH36YqKgoXF1dGTZsGMOGDaNu3bq4urrSqlUr3n33XebNm8e9995LWFgYjz76KFlZWbZbE0yaNInw8HCOHDnC2LFj+fzzzwGIjY0lIiKCkJAQxo0bZ7vFcFBQEFOnTuWee+4hJCSEhISESvjUlLo6DfQaIPOnc5z++w5Op/yZ/Ly7+HlFE5ySPUCgZ4A7ox+uxV3+ThR/V1nKsWNnKr2+y4FYVkpKCtnZRXeIdHFxAeCHH34gJyeH4cOH4+TkRFZWFlarlcaNG/Ptt98SFRXFsGHD+OSTT/jPf/5DdHQ0zZs359tvv7VdtRoQEMCAAQMYOXIkjzzyCDt37mTPnj20adOGDz/88Kq3JoCiUynHjh1LTEwM//3vf7Farbz33nu25/39/fnxxx/5/e9/z8yZM1GqKmmgO7jMn86R/uUhCtJzcU1O4GJWOgVZVnp718LFGO5K9WbZ6mxyC4suBKI4OG8XhYWFAOTn5wNgtVoRET766COMMRQWFpKfn8/OnTtxcXHhyy+/ZPPmzRQWFlJQUMArr7xCSkoKhYWFfPXVV7Rt25aUlBTmz59P48aNmTdvHv7+/oSEhPDxxx/zwQcf8Mgjj1y1noMHDxIcHEyrVq2AKy94uvzaywuGKFWVNNAd3IXViUh+USh6HVnDPe4eDDz6C9szL+FmDJ+kZnA2w4lL6UWjZFO/UXWWa7fc3FxbyF928eJFrFYrnp6eGGOoVasWVquVlJQULl26REFBAQ0aNMBisZCdnY2I8Nprr3Hx4kUWLVrExIkT+fnnn/ntb3971fe9PD10NW5ubkDRgiFWq/XWO6rUDdDz0B1cQfqv0xmSncaMhg0B2Jx5idjMTI7l55Hv6opTUHMKjh5Gzpy62qHuGD/++CMAFy5cKLXfarXy7LPP2rYtFguFhYXk5eVx77332kJ4zJgx5OTkEBsbC8CQIUNsr+natSt33XUXGzZsYMqUKVgsFgYPHsyQIUNsF0EtXbqUJ5980q7l+RITExkwYAB79+4ttb9Xr17MnDnTdrGVPd5//308PT154okn7H6NcizmeiOOyhIZGSm6Qk3lO/33HbZQv7R6MpKddkWbPv/6N7l74zj//Dg8R/8O65FD5G35vqpLve0ZYxARmjRpwvHjx23777nnHn788UdcXFzIz8/HycmJgoICAPr370/Pnj15/fXX2bJlC23bti11zKFDh7J7924OHz7Mhg0bcHV1pWPHjtx///0MGDCA/v37X/Gaklq0aMGgQYN48803K6fT6rZjjNktIuX+n15H6A7Op28Q6V8eQvILcW07mNy4j6Hg128/CyzO+F3MISWgHk5Ng5H08xScPIFxdsVJwFpQzjelNdTlwU/JMIdffyO4PAV0OcwBVq1axapVqwCIiIhg/PjxLFiwgEuXLgFQp07RVbr9+vVj+/btXLhwARcXFywWC6mpqQQFBTF58mTWr1+PxWLB2dmZp556ipCQEGbNmsWJEydIT08HYNq0aXh6evLyyy9z+PBhxo8fT3JyMk5OTixbtqzUl7vKMekIvQbI/OkcF1YnYk3PIfP4RnL3fYlLbjY5rrX4pfkjrL23G98EZnBmyvP4Lyg6Nc/6yEOkZ5wi0MuLxEsXq7kHjunylE9Jl38LuCwgIAAvLy+SkpJwdna23aGyYcOGxMbG0q1bN0SEMWPG8K9//QtjDBcvXqRr165MnjyZwYMHk5OTQ2FhIZ6enlXaP1U5bnmEbozpB8wCnID5IvL3Ms+7AR8BHYBUYKiIJN5K0arieEXUxSuibvFWD+CP/Lz9DDs+PkCBVQg5nkfDrBz+JmBEqJcjeNZrz54Lpzhnzb/WodUtKBvmUPRbwN13382xY8ewWq1YrVYGDx7M3LlzAahduzY5OTnUq1ePZ555BldXVy5dusSCBQvw9vbmlVdeITs7m5MnTzJ48GAA3N3dq7Rfqvpc9ywXY4wTMAf4DdAWGG6MKTup9yRwXkRaAG8D/6joQlXFatWpPn1Gt8G7TtFZGSHpHhx4fB4711zim42Z9G4bDSK466jull1vZGyMKbV97Ngx222Hc3NziYmJoWHDhmRmZnL+/Hmys7PZu3cvp0+fto3IPT09mTBhwhUjfFWz2HPaYkfgsIj8IiJ5wFKg7FI3g4DFxY8/B6JM2X+l6rbTqlN9xrzelWff78Mvyf9iZ8oqMvMzEIToRi0YXq8xoqfe3bLLV7uW5e3tDYCzc9EvysYYjDFs2bIFi8VC9+7dee2113B3d+fQoUPUqlWLunXr4uvrS926dTlw4IDt9ampqbazbA4cOMD58+cJDg5mxYoV5ObmXrUG5VjsCfRGwIkS20nF+8ptIyJWIAO4q+yBjDFPG2N2GWN2JScn31zFqlK4+/hxPPMA3yS9z8EHfktTtyw8ej/FP8Ijqrs0h3X5i9HL56tfvuVB165dcXFxYd++fSQmJtKyZUvb840bN+bixaLvNPbs2QMUTak8+OCDvP/++1y4cIFOnToRFxdHcHAwf/zjH+nSpQtnzlT+Fb+q+tkT6OWNtMv+TmdPG0RkrohEikhkQECAPfWpKtLniSexOBddJZqX44E430VEky78ENiH7vO60vTlptVcoeO5/Evs3XffjZubGz4+Pjg7O5OdnU1ubi4ZGRnExMRw9uxZoqOjyc7OJj4+HldXV86dO8fy5cvx8PCgSZMmjBkzhpEjR/Luu++SlpZGy5YtWbduHfHx8ezevVtvO1xDXPcsF2PMfcA0EelbvP0qgIj8rUSb1cVtthpjnIEzQIBc4+B6lsvt58Cm9Wxa+hGetY/RgwlYrAHM8ClgfcOJiAHrJSsJE/SGUyUZY3B2dradsujh4UFeXh4FBQU0a9aMlJQULly4gMVioU6dOnh7e3Pq1Cl8fHx46KGH+PbbbxERkpOT2b59+w1dSKRqpmud5WLPCH0n0NIYE2yMcQWGASvKtFkBjCl+PARYd60wV7enNt178/SchYz6yzrOua1DTC6TLjjhle9D/vl8jkw7Ut0l3rDLi1VbLL/+U3dycsLJyalUu4YNG9pGuy4uLvTo0YP/+Z//QUQ4fPgwTZs2Zc2aNURHR5e6XWlhYSEjRoxg2bJl9OzZk1atWtGqVSsWLlzIkSNHyMjIQEQoKCggOTmZo0ePkpubS3JyMgsWLOD06dOcOXOGgoICDXN16652X92SP0B/4GfgCPBa8b7pwMDix+7AMuAwsANodr1j6v3Qb3973/yLHJnypTw/dbCEfRgu7Re1l/aL2gvOpe9r7l3XU5yKF6K4vDiFu7u71KpVS4wxpdqW/fHz85PVq1dL586dBRAfHx954IEHxNvbu+jY3t4SHBx8xevc3d2lQYMGtuNfXgijVq1acvDgwXL7c/78eWnZsqUMGTKk1P6mTZtKcnJyqX3vvfeeBAQESGhoqISEhMiqVasq7XNW6kZwjfuh64VF6royfzrH5K9/x9ZGqeS6XqAw3w/L2QfpmHQXbQN38NJri6q7RKVqDL30X90Sr4i6vBtRdpbtsmeqtBal1NXp7XOVUspBaKArpZSD0EBXSikHoYGulFIOQgNdKaUchAa6Uko5CA10pZRyEBroSinlIKrtSlFjTDJwrFrevPL5AynVXUQl0v7d2bR/d7amIlLu7WqrLdAdmTFm19UuzXUE2r87m/bPcemUi1JKOQgNdKWUchAa6JVjbnUXUMm0f3c27Z+D0jl0pZRyEDpCV0opB6GBrpRSDkIDvQIYY+oYY/7PGHOo+M/a5bQJN8ZsNcbsM8bEG2OGVket9jLG9DPGHDTGHDbGTC7neTdjTEzx89uNMUFVX+WtsaOPLxpj9hf/fcUaY5pWR50363r9K9FuiDFGjDF31Kl+9vTPGPN48d/hPmPMv6u6xip3tbXp9Mf+H+ANYHLx48nAP8pp0wpoWfy4IXAa8Kvu2q/SHyeK1o9tBrgCe4C2Zdo8A7xf/HgYEFPddVdCH3sDnsWPf38n9dGe/hW3qwVsBLYBkdVddwX//bUEfgJqF2/Xre66K/tHR+gVYxCwuPjxYuDhsg1E5GcROVT8+BRwDij3aq/bQEfgsIj8IiJ5wFKK+lhSyT5/DkQZY0wV1nirrttHEVkvIlnFm9uAxlVc462w5+8Q4M8UDUhyqrK4CmBP/54C5ojIeQAROVfFNVY5DfSKUU9ETgMU/1n3Wo2NMR0pGlUcqYLabkYj4ESJ7aTifeW2ERErkAHcVSXVVQx7+ljSk8C3lVpRxbpu/4wxEUCgiHxTlYVVEHv+/loBrYwxPxhjthlj+lVZddVEF4m2kzFmLVC/nKdeu8HjNAA+BsaISGFF1FYJyhtplz2/1Z42tzO76zfGjAIigZ6VWlHFumb/jDEW4G1gbFUVVMHs+ftzpmjapRdFv11tMsa0F5H0Sq6t2mig20lE7r/ac8aYs8aYBiJyujiwy/3VzhjjA/wH+KOIbKukUitCEhBYYrsxcOoqbZKMMc6AL5BWNeVVCHv6iDHmfor+p91TRHKrqLaKcL3+1QLaAxuKZ8rqAyuMMQNFZFeVVXnz7P03uk1E8oGjxpiDFAX8zqopserplEvFWAGMKX48BlhetoExxhX4CvhIRJZVYW03YyfQ0hgTXFz3MIr6WFLJPg8B1knxN093iOv2sXhK4gNg4B04/3rN/olIhoj4i0iQiARR9B3BnRLmYN+/0a8p+mIbY4w/RVMwv1RplVVMA71i/B14wBhzCHigeBtjTKQxZn5xm8eBHsBYY0xc8U949ZR7bcVz4hOA1cAB4DMR2WeMmW6MGVjc7EPgLmPMYeBFis7uuWPY2ccZgDewrPjvq2xg3Lbs7N8dy87+rQZSjTH7gfXAJBFJrZ6Kq4Ze+q+UUg5CR+hKKeUgNNCVUspBaKArpZSD0EBXSikHoYGulFIOQgNdKaUchAa6Uko5iP8HQRkWUuQ+2OgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00306013, -0.0039827 , -0.00286421, ..., -0.0100321 ,\n",
       "         0.00739379, -0.00465176],\n",
       "       [ 0.00056763,  0.00415413,  0.00394424, ...,  0.0005764 ,\n",
       "        -0.0011694 , -0.00293623],\n",
       "       [ 0.0040359 , -0.01179888, -0.0038721 , ..., -0.00105566,\n",
       "         0.007907  , -0.00446876],\n",
       "       ...,\n",
       "       [ 0.01445384, -0.00262856,  0.02708466, ..., -0.00205736,\n",
       "        -0.00812906, -0.00118672],\n",
       "       [-0.00298161, -0.00106615, -0.00029968, ...,  0.26879288,\n",
       "        -0.00202195, -0.0075813 ],\n",
       "       [-0.00902833, -0.00076783, -0.02092184, ...,  0.16138977,\n",
       "        -0.07258559, -0.00284373]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['outlier_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0077383 , -0.00649281, -0.00328674, ..., -0.00991542,\n",
       "        -0.00068425, -0.01026424],\n",
       "       [-0.0054272 , -0.00467697,  0.00491095, ..., -0.01497707,\n",
       "         0.00397596, -0.00341377],\n",
       "       [-0.00168579, -0.00334015,  0.00881591, ..., -0.01773263,\n",
       "        -0.00046109, -0.00265123],\n",
       "       ...,\n",
       "       [ 0.00395822,  0.00149832,  0.00950041, ..., -0.00229025,\n",
       "        -0.01839082, -0.006462  ],\n",
       "       [ 0.00294663, -0.00310521, -0.00017864, ..., -0.00812701,\n",
       "        -0.00052146, -0.00241429],\n",
       "       [ 0.00261461, -0.00164788,  0.00479076, ...,  0.00386268,\n",
       "        -0.00316197, -0.00555313]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_fraction = 0.15\n",
    "Nsamples_unreliable = 280*resample_fraction\n",
    "\n",
    "resample_dict = {\n",
    "    -1: 42,\n",
    "    1: 280\n",
    "}\n",
    "\n",
    "underSample = RandomUnderSampler(sampling_strategy = resample_dict,\n",
    "                                 random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imb, y_imb = underSample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', OneClassSVM(nu = resample_fraction))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv, scoring = 'f1_macro')\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X_imb,\n",
    "                        y = y_imb,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1_macro', 'precision_macro', 'recall_macro'],\n",
    "                        return_estimator = True,\n",
    "                        return_train_score = True)\n",
    "\n",
    "test_auc = scores['test_roc_auc']\n",
    "train_auc = scores['train_roc_auc']\n",
    "\n",
    "test_accuracy = scores['test_accuracy']\n",
    "train_accuracy = scores['train_accuracy']\n",
    "\n",
    "test_f1 = scores['test_f1_macro']\n",
    "train_f1 = scores['train_f1_macro']\n",
    "\n",
    "test_precision = scores['test_precision_macro']\n",
    "train_precision = scores['train_precision_macro']\n",
    "\n",
    "test_recall = scores['test_recall_macro']\n",
    "train_recall = scores['train_recall_macro']\n",
    "\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.294890873015873"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.692548076923077"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4698809688603561"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4817532332081769"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46765873015873016"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
