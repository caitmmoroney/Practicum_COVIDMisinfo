{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "#from sklearn.covariance import EllipticEnvelope\n",
    "#from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a19444210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outlier target values\n",
    "outlier = []\n",
    "for i in tweets['Is_Unreliable']:\n",
    "    if i == 0:\n",
    "        i = 1\n",
    "    else:\n",
    "        i = -1\n",
    "    outlier.append(i)\n",
    "tweets['outlier_target'] = outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>outlier_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus is spreading wild wide and cities ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This morning, Sunnybrook discharged home the p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This afternoon, @WHO declared #coronavirus a p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinese health authorities announced Sunday th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local communities band together to show their ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable Category  \\\n",
       "280              0      NaN   \n",
       "281              0      NaN   \n",
       "282              0      NaN   \n",
       "283              0      NaN   \n",
       "284              0      NaN   \n",
       "..             ...      ...   \n",
       "555              0      NaN   \n",
       "556              0      NaN   \n",
       "557              0      NaN   \n",
       "558              0      NaN   \n",
       "559              0      NaN   \n",
       "\n",
       "                                                 Tweet  outlier_target  \n",
       "280  Coronavirus is spreading wild wide and cities ...               1  \n",
       "281  This morning, Sunnybrook discharged home the p...               1  \n",
       "282  This afternoon, @WHO declared #coronavirus a p...               1  \n",
       "283  Chinese health authorities announced Sunday th...               1  \n",
       "284  Local communities band together to show their ...               1  \n",
       "..                                                 ...             ...  \n",
       "555  BREAKING: Harvard classes will move online sta...               1  \n",
       "556  Singularity University is hosting a FREE Virtu...               1  \n",
       "557  Coronavirus: how does it spread and what are t...               1  \n",
       "558  Stanford just cancelled classes for the rest o...               1  \n",
       "559  Tech conferences were cancelled in #Waterloo R...               1  \n",
       "\n",
       "[280 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reliable_tweets = tweets[tweets['outlier_target'] == 1]\n",
    "reliable_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15,\n",
    "                   lowercase = True,\n",
    "                   lemmatize = True,\n",
    "                   pmi = True,\n",
    "                   pmi_positive = True,\n",
    "                   sppmi_k = 5,\n",
    "                   laplace_smoothing = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.fit_transform(reliable_tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yet</th>\n",
       "      <th>yokohama</th>\n",
       "      <th>york</th>\n",
       "      <th>yorku</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>1.087979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.859152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.940618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.393794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows × 1164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   !         #         (         )         ,    -   --  \\\n",
       "!           1.087979  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "#           0.000000  1.859152  0.000000  0.000000  0.146213  0.0  0.0   \n",
       "(           0.000000  0.000000  0.000000  1.098284  0.000000  0.0  0.0   \n",
       ")           0.000000  0.000000  1.098284  0.000000  0.000000  0.0  0.0   \n",
       ",           0.000000  0.146213  0.000000  0.000000  0.433185  0.0  0.0   \n",
       "...              ...       ...       ...       ...       ...  ...  ...   \n",
       "zone        0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "zuckerberg  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "—           0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "‘           0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "’           0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "\n",
       "                   .  ...    1  ...  yeah  yet  yokohama  york  yorku  zone  \\\n",
       "!           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "#           0.940618  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "(           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       ")           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       ",           0.393794  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "...              ...  ...  ...  ...   ...  ...       ...   ...    ...   ...   \n",
       "zone        0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "zuckerberg  0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "—           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "‘           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "’           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "\n",
       "            zuckerberg    —    ‘    ’  \n",
       "!                  0.0  0.0  0.0  0.0  \n",
       "#                  0.0  0.0  0.0  0.0  \n",
       "(                  0.0  0.0  0.0  0.0  \n",
       ")                  0.0  0.0  0.0  0.0  \n",
       ",                  0.0  0.0  0.0  0.0  \n",
       "...                ...  ...  ...  ...  \n",
       "zone               0.0  0.0  0.0  0.0  \n",
       "zuckerberg         0.0  0.0  0.0  0.0  \n",
       "—                  0.0  0.0  0.0  0.0  \n",
       "‘                  0.0  0.0  0.0  0.0  \n",
       "’                  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1164 rows x 1164 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164, 1164)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00362609, -0.0072365 ],\n",
       "       [ 0.31869285,  0.31819305],\n",
       "       [-0.01368568,  0.04213058],\n",
       "       ...,\n",
       "       [-0.0011362 , -0.00272515],\n",
       "       [-0.0011362 , -0.00272515],\n",
       "       [-0.0011362 , -0.00272515]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.003626</td>\n",
       "      <td>-0.007236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.318693</td>\n",
       "      <td>0.318193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>-0.013686</td>\n",
       "      <td>0.042131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>-0.013406</td>\n",
       "      <td>0.041760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.088350</td>\n",
       "      <td>0.176274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>-0.001136</td>\n",
       "      <td>-0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>-0.001136</td>\n",
       "      <td>-0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—</th>\n",
       "      <td>-0.001136</td>\n",
       "      <td>-0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>-0.001136</td>\n",
       "      <td>-0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>-0.001136</td>\n",
       "      <td>-0.002725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comp 1    Comp 2\n",
       "!           0.003626 -0.007236\n",
       "#           0.318693  0.318193\n",
       "(          -0.013686  0.042131\n",
       ")          -0.013406  0.041760\n",
       ",           0.088350  0.176274\n",
       "...              ...       ...\n",
       "zone       -0.001136 -0.002725\n",
       "zuckerberg -0.001136 -0.002725\n",
       "—          -0.001136 -0.002725\n",
       "‘          -0.001136 -0.002725\n",
       "’          -0.001136 -0.002725\n",
       "\n",
       "[1164 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.33323760e-02,  1.71142204e-01,  1.18486737e-02, ...,\n",
       "        -4.40297293e-02, -2.19837161e-02,  6.34492280e-02],\n",
       "       [-4.51325827e-01,  4.15786369e-01,  4.09569011e-01, ...,\n",
       "        -4.67079656e-01, -1.82536695e-01,  1.31594980e-01],\n",
       "       [-1.91926413e-01, -1.39404671e-01,  8.80510663e-02, ...,\n",
       "        -2.43153739e-02,  2.34440234e-01, -7.03093169e-02],\n",
       "       ...,\n",
       "       [-2.35121772e-03,  6.55753657e-04, -5.31817853e-05, ...,\n",
       "        -1.07305019e-04, -3.91730983e-03, -2.71475123e-04],\n",
       "       [-2.35121772e-03,  6.55753657e-04, -5.31817853e-05, ...,\n",
       "        -6.91580460e-05, -3.91730983e-03, -2.71475123e-04],\n",
       "       [-2.35121772e-03,  6.55753657e-04, -5.31817853e-05, ...,\n",
       "        -6.91580460e-05, -3.91730983e-03, -2.71475123e-04]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['outlier_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02230361,  0.06598817,  0.10717643, ..., -0.06743213,\n",
       "         0.03323632,  0.09534925],\n",
       "       [-0.03020868,  0.05257156,  0.09428937, ..., -0.06179986,\n",
       "         0.03021533,  0.08204715],\n",
       "       [-0.03858576,  0.07014131,  0.12594373, ..., -0.08050502,\n",
       "         0.04081637,  0.10909335],\n",
       "       ...,\n",
       "       [-0.0802404 ,  0.020333  ,  0.08216534, ..., -0.0436982 ,\n",
       "         0.06149434,  0.09955777],\n",
       "       [-0.04041838,  0.07109355,  0.07953549, ..., -0.08537166,\n",
       "         0.00208996,  0.06563886],\n",
       "       [-0.06944513,  0.02638856,  0.02366254, ..., -0.01566924,\n",
       "        -0.01088447,  0.00138766]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_fraction = 0.20\n",
    "Nsamples_unreliable = int(np.round(280/(1-resample_fraction)*resample_fraction))\n",
    "\n",
    "resample_dict = {\n",
    "    -1: Nsamples_unreliable,\n",
    "    1: 280\n",
    "}\n",
    "\n",
    "underSample = RandomUnderSampler(sampling_strategy = resample_dict,\n",
    "                                 random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imb, y_imb = underSample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', OneClassSVM(nu = resample_fraction))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv, scoring = 'f1_macro')\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X_imb,\n",
    "                        y = y_imb,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1_macro', 'precision_macro', 'recall_macro'],\n",
    "                        return_estimator = True,\n",
    "                        return_train_score = True)\n",
    "\n",
    "test_auc = scores['test_roc_auc']\n",
    "train_auc = scores['train_roc_auc']\n",
    "\n",
    "test_accuracy = scores['test_accuracy']\n",
    "train_accuracy = scores['train_accuracy']\n",
    "\n",
    "test_f1 = scores['test_f1_macro']\n",
    "train_f1 = scores['train_f1_macro']\n",
    "\n",
    "test_precision = scores['test_precision_macro']\n",
    "train_precision = scores['train_precision_macro']\n",
    "\n",
    "test_recall = scores['test_recall_macro']\n",
    "train_recall = scores['train_recall_macro']\n",
    "\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5405612244897959"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7285714285714286"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5800235532966089"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5787799867092989"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5839285714285715"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
