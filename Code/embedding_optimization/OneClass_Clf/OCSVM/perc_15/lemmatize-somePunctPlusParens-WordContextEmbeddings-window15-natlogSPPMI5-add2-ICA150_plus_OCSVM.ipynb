{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "#from sklearn.covariance import EllipticEnvelope\n",
    "#from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a1f9f0d10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outlier target values\n",
    "outlier = []\n",
    "for i in tweets['Is_Unreliable']:\n",
    "    if i == 0:\n",
    "        i = 1\n",
    "    else:\n",
    "        i = -1\n",
    "    outlier.append(i)\n",
    "tweets['outlier_target'] = outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>outlier_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus is spreading wild wide and cities ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This morning, Sunnybrook discharged home the p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This afternoon, @WHO declared #coronavirus a p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinese health authorities announced Sunday th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local communities band together to show their ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable Category  \\\n",
       "280              0      NaN   \n",
       "281              0      NaN   \n",
       "282              0      NaN   \n",
       "283              0      NaN   \n",
       "284              0      NaN   \n",
       "..             ...      ...   \n",
       "555              0      NaN   \n",
       "556              0      NaN   \n",
       "557              0      NaN   \n",
       "558              0      NaN   \n",
       "559              0      NaN   \n",
       "\n",
       "                                                 Tweet  outlier_target  \n",
       "280  Coronavirus is spreading wild wide and cities ...               1  \n",
       "281  This morning, Sunnybrook discharged home the p...               1  \n",
       "282  This afternoon, @WHO declared #coronavirus a p...               1  \n",
       "283  Chinese health authorities announced Sunday th...               1  \n",
       "284  Local communities band together to show their ...               1  \n",
       "..                                                 ...             ...  \n",
       "555  BREAKING: Harvard classes will move online sta...               1  \n",
       "556  Singularity University is hosting a FREE Virtu...               1  \n",
       "557  Coronavirus: how does it spread and what are t...               1  \n",
       "558  Stanford just cancelled classes for the rest o...               1  \n",
       "559  Tech conferences were cancelled in #Waterloo R...               1  \n",
       "\n",
       "[280 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reliable_tweets = tweets[tweets['outlier_target'] == 1]\n",
    "reliable_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15,\n",
    "                   lowercase = True,\n",
    "                   lemmatize = True,\n",
    "                   pmi = True,\n",
    "                   pmi_positive = True,\n",
    "                   sppmi_k = 5,\n",
    "                   laplace_smoothing = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.fit_transform(reliable_tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yet</th>\n",
       "      <th>yokohama</th>\n",
       "      <th>york</th>\n",
       "      <th>yorku</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.564601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.040209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.440281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows × 1164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   !         #         (         )         ,    -   --  \\\n",
       "!           0.564601  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "#           0.000000  2.040209  0.000000  0.000000  0.257241  0.0  0.0   \n",
       "(           0.000000  0.000000  0.000000  0.664903  0.000000  0.0  0.0   \n",
       ")           0.000000  0.000000  0.664903  0.000000  0.000000  0.0  0.0   \n",
       ",           0.000000  0.257241  0.000000  0.000000  0.458174  0.0  0.0   \n",
       "...              ...       ...       ...       ...       ...  ...  ...   \n",
       "zone        0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "zuckerberg  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "—           0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "‘           0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "’           0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "\n",
       "                   .  ...    1  ...  yeah  yet  yokohama  york  yorku  zone  \\\n",
       "!           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "#           1.062491  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "(           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       ")           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       ",           0.440281  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "...              ...  ...  ...  ...   ...  ...       ...   ...    ...   ...   \n",
       "zone        0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "zuckerberg  0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "—           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "‘           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "’           0.000000  0.0  0.0  ...   0.0  0.0       0.0   0.0    0.0   0.0   \n",
       "\n",
       "            zuckerberg    —    ‘    ’  \n",
       "!                  0.0  0.0  0.0  0.0  \n",
       "#                  0.0  0.0  0.0  0.0  \n",
       "(                  0.0  0.0  0.0  0.0  \n",
       ")                  0.0  0.0  0.0  0.0  \n",
       ",                  0.0  0.0  0.0  0.0  \n",
       "...                ...  ...  ...  ...  \n",
       "zone               0.0  0.0  0.0  0.0  \n",
       "zuckerberg         0.0  0.0  0.0  0.0  \n",
       "—                  0.0  0.0  0.0  0.0  \n",
       "‘                  0.0  0.0  0.0  0.0  \n",
       "’                  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1164 rows x 1164 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164, 1164)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00080343, -0.00285963],\n",
       "       [ 0.09901409,  0.46637523],\n",
       "       [ 0.01666011,  0.01562184],\n",
       "       ...,\n",
       "       [ 0.00068219, -0.00255867],\n",
       "       [ 0.00068219, -0.00255867],\n",
       "       [ 0.00068219, -0.00255867]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.000803</td>\n",
       "      <td>-0.002860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.099014</td>\n",
       "      <td>0.466375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.016660</td>\n",
       "      <td>0.015622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.016899</td>\n",
       "      <td>0.015630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.040919</td>\n",
       "      <td>0.204408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—</th>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.002559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comp 1    Comp 2\n",
       "!           0.000803 -0.002860\n",
       "#           0.099014  0.466375\n",
       "(           0.016660  0.015622\n",
       ")           0.016899  0.015630\n",
       ",          -0.040919  0.204408\n",
       "...              ...       ...\n",
       "zone        0.000682 -0.002559\n",
       "zuckerberg  0.000682 -0.002559\n",
       "—           0.000682 -0.002559\n",
       "‘           0.000682 -0.002559\n",
       "’           0.000682 -0.002559\n",
       "\n",
       "[1164 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.12568595e-04, -1.25106845e-02,  1.10757438e-02, ...,\n",
       "         2.72326151e-02,  6.82444095e-03,  1.34258307e-02],\n",
       "       [ 7.46250187e-01,  3.10766707e-01, -2.68977956e-01, ...,\n",
       "        -7.60711970e-01,  9.44209372e-02, -6.64944065e-01],\n",
       "       [ 6.50446901e-02,  6.14878253e-02,  2.83275232e-01, ...,\n",
       "         1.68564900e+00,  5.25013359e-01, -8.30649402e-02],\n",
       "       ...,\n",
       "       [-1.75032144e-03, -2.14690058e-03,  2.32596712e-03, ...,\n",
       "         1.47837794e-03,  7.49656894e-04,  2.22818359e-03],\n",
       "       [-1.75032144e-03, -2.14690058e-03,  2.32596712e-03, ...,\n",
       "         1.47837794e-03,  7.49656894e-04,  2.22818359e-03],\n",
       "       [-1.75032144e-03, -2.14690058e-03,  2.32596712e-03, ...,\n",
       "         1.47837794e-03,  7.49656894e-04,  2.22818359e-03]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['outlier_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13887834,  0.19129612, -0.07263979, ..., -0.13616514,\n",
       "        -0.01683884, -0.13278155],\n",
       "       [ 0.12184415,  0.16544976, -0.08470718, ..., -0.11941941,\n",
       "        -0.02788137, -0.13414143],\n",
       "       [ 0.16304231,  0.22131532, -0.11371823, ..., -0.15971867,\n",
       "        -0.03742505, -0.17959797],\n",
       "       ...,\n",
       "       [ 0.13474486,  0.14653109, -0.04873009, ..., -0.11314082,\n",
       "        -0.04649864, -0.13930001],\n",
       "       [ 0.14956745,  0.15903257, -0.05475308, ..., -0.13316671,\n",
       "         0.00693726, -0.13907932],\n",
       "       [ 0.03342529,  0.08915759, -0.08771962, ..., -0.07989709,\n",
       "        -0.04948916, -0.09024373]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_fraction = 0.15\n",
    "Nsamples_unreliable = int(np.round(280/(1-resample_fraction)*resample_fraction))\n",
    "\n",
    "resample_dict = {\n",
    "    -1: Nsamples_unreliable,\n",
    "    1: 280\n",
    "}\n",
    "\n",
    "underSample = RandomUnderSampler(sampling_strategy = resample_dict,\n",
    "                                 random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imb, y_imb = underSample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', OneClassSVM(nu = resample_fraction))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv, scoring = 'f1_macro')\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X_imb,\n",
    "                        y = y_imb,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1_macro', 'precision_macro', 'recall_macro'],\n",
    "                        return_estimator = True,\n",
    "                        return_train_score = True)\n",
    "\n",
    "test_auc = scores['test_roc_auc']\n",
    "train_auc = scores['train_roc_auc']\n",
    "\n",
    "test_accuracy = scores['test_accuracy']\n",
    "train_accuracy = scores['train_accuracy']\n",
    "\n",
    "test_f1 = scores['test_f1_macro']\n",
    "train_f1 = scores['train_f1_macro']\n",
    "\n",
    "test_precision = scores['test_precision_macro']\n",
    "train_precision = scores['train_precision_macro']\n",
    "\n",
    "test_recall = scores['test_recall_macro']\n",
    "train_recall = scores['train_recall_macro']\n",
    "\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5034920634920635"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7597668997668998"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5456561202179069"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5521464168480357"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5494444444444445"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__kernel': 'rbf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
