\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}

\title{Automated Detection of Misinformation in Tweets about COVID-19}

\author{
    Caitlin Moroney
    \thanks{Use footnote for providing further information about author (webpage,
alternative address)---\emph{not} for acknowledging funding agencies.
Optional.}
   \\
    Department of Mathematics and Statistics \\
    American University \\
  Washington, DC 20016 \\
  \texttt{\href{mailto:cm0246b@american.edu}{\nolinkurl{cm0246b@american.edu}}} \\
  }


% Pandoc citation processing

\begin{document}
\maketitle

\def\tightlist{}


\begin{abstract}
Enter the text of your abstract here.
\end{abstract}

\keywords{
    COVID-19
   \and
    coronavirus
   \and
    NLP
   \and
    machine learning
   \and
    misinformation
  }

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Insert introduction text here.

The rest of the paper is organized as follows: Section \ref{sec:methods}
discusses our methodology; Section \ref{sec:results} presents the
results of our experiments; Section \ref{sec:discuss} discusses our
results and plans for future work.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

\label{sec:methods} This paper explores a series of methods which seek
to exploit linguistic features in raw text data in order to perform the
automated detection of unreliable tweets. The experiments follow the
same general framework with certain implementation details tweaked for
each experiment. The first step in this framework is the application of
NLP featurization methods to the raw tweet text. While different
featurization methods are compared, all methods involve the use of NLP
tools to represent the text with numeric features. Subsequently, latent
variable methods are employed to reduce the dimensionality of the
resulting \(\mathbf{X}\) feature matrix (as well as to uncover latent
variables). The latent variables are then used in the classification
task. Finally, we evaluate the classification algorithms paired with the
featurization methods with respect to performance and explainability. We
use the typical performance metrics, including accuracy, F-score,
precision, recall, and ROC-AUC. We use LIME to evaluate local
explainability for non-interpretable methods. Furthermore, we present a
new explainability framework for latent variable methods as well as a
new explainability metric. Below, we explore in greater detail the
methods used for featurization, latent variables, classification, and
evaluation of explainability.

\hypertarget{nlp-featurization}{%
\subsection{NLP featurization}\label{nlp-featurization}}

In order to obtain features from the raw tweet text, we first employed
standard preprocessing, to include removing stop words and punctuation
as well as lemmatizing words. The two approaches we pursued involved (1)
Bag-of-Words and (2) word embeddings, each followed by latent variable
methods.

\hypertarget{bag-of-words}{%
\subsubsection{Bag-of-Words}\label{bag-of-words}}

\begin{itemize}
\tightlist
\item
  Bag-of-Words + topic modeling

  \begin{itemize}
  \tightlist
  \item
    Vectorization: raw counts, tf-idf, binary
  \item
    PCA + ICA
  \end{itemize}
\end{itemize}

After constructing the Bag-of-Words matrix, we employed topic modeling
to reduce the dimensionality of the \(Nxp\) matrix.

\hypertarget{word-embeddings}{%
\subsubsection{Word embeddings}\label{word-embeddings}}

To create word embeddings, we used the word-context co-occurrence matrix
which, unlike the Bag-of-Words method, is able to incorporate
information about context from the raw text data. We trained the
embeddings on the full set of 560 tweets. With the Bag-of-Words methods,
we are able to encode information about words' presence in a given
document; the co-occurrence approach improves upon this by allowing us
to look at word usage with respect to the presence of other words. We
constructed a word-context co-occurrence matrix using the entire
vocabulary for both target terms and context terms. In other words, the
matrix was symmetric. We incorporated a number of hyperparameters
related to the construction and subsequent transformation of this
matrix, including context window size, the use of raw counts or
variations on the Pointwise Mutual Information (PMI), and Laplace
smoothing. We also included ``\textless START\textgreater{}'' and
``\textless END\textgreater{}'' tokens at the beginning and end of each
tweet.

\begin{itemize}
\tightlist
\item
  Context window size
\end{itemize}

Window size refers to the number of tokens before and after the target
word are scanned for context words. According to some sources, a window
size of four or larger captures semantic representations of the words,
whereas smaller windows capture more syntactic representations.

\begin{itemize}
\tightlist
\item
  Raw counts vs PMI (or PPMI)
\item
  Shifted vs unshifted PMI (or PPMI)
\item
  Laplace smoothing
\item
  Use of start \& end tokens
\end{itemize}

Latent variable methods were subsequently applied to the word embeddings
in order to reduce the dimensionality.

Finally, we averaged over the word embeddings for the words in each
tweet to obtain a single vector representation for each tweet.

Overall, it seemed that larger context windows prevailed - a window size
of 15 +/- performed the best (I tried window sizes of 1, 2, 4, 6, 10,
15, and 20). I find this interesting because tweets are such short posts
(perhaps 30 words encompasses the entirety of the tweet for most
tweets). Incorporating add-one Laplace smoothing and shifted {[}P{]}PMI
decreased performance across all metrics (accuracy, precision, recall,
ROC AUC, and F1 score). Interestingly, PMI often outperformed not only
raw frequencies but also PPMI (positive PMI). However, with other
optimal hyperparameters held constant, there was virtually no difference
in performance between PMI and PPMI. Optimal text cleaning included
removing special characters which were not punctuation (parentheses,
question marks, exclamation points, periods, commas, hyphens, colons,
and semicolons) or alphanumeric characters (letters or numbers),
converting all text to lowercase, removing stop words, and lemmatizing
words (using NLTK's WordNetLemmatizer aided by NLTK's part-of-speech
tagger).

\textbf{The best results included ROC AUC of 0.94, accuracy of 0.89, F1
score of 0.90, precision of 0.86, and recall of 0.94. These scores are
all higher than the best results from last week and were obtained using
the same nested CV procedure (3 folds for inner loop; 5 folds for outer
loop).} NOTE: These should be replaced with the correct numbers.

\begin{itemize}
\tightlist
\item
  BERT embeddings from pre-trained model
\end{itemize}

\hypertarget{latent-variable-methods}{%
\subsection{Latent variable methods}\label{latent-variable-methods}}

In order to reduce the dimensionality of the data matrix \(\mathbf{X}\),
we employed latent variable methods. Specifically, we applied Principal
Component Analysis (PCA) followed by Independent Component Analysis
(ICA) to both the Bag-of-Words matrix and the word-context co-occurrence
matrix.

In order to perform PCA, we rescale the data matrix and then apply
Singular Value Decomposition (SVD). We first scale the data so that the
columns have zero mean and unit variance. The SVD model is as follows:
\[\mathbf{X = U \Sigma V}^T\] where \(\mathbf{\Sigma}\) is a diagonal
matrix containing the singular values of \(\mathbf{X}\), \(\mathbf{U}\)
is BLAH, and \(\mathbf{V}^T\) is BLAHBLAH.

To achieve PCA, we can use truncated SVD. In essence, we perform SVD and
keep only the columns of \(\mathbf{U}\) and \(\mathbf{V}^T\) that
correspond to the largest \(k\) singular values in the diagonal matrix
\(\mathbf{\Sigma}\), where \(k\) is the desired order for the
approximation of our initial data matrix. Therefore, if \(k\) is less
than \(m\), we have achieved dimensionality reduction.

\begin{itemize}
\tightlist
\item
  Explain ICA
\end{itemize}

The ICA model can be represented in matrix form as follows:
\[\mathbf{X = AS}\]

The pipeline involves performing SVD on the initial data matrix,
\(\mathbf{X}\), such that we can obtain the \(\mathbf{U}\) matrix which
contains the SVD feature vectors. This matrix is used as the input for
ICA, so that we have \(\mathbf{U = AS}\). Then, \(\mathbf{S}\) is the
whitened mixing matrix, and \(\mathbf{A}\) contains the ICA features.
This process is represented visually in Figure \ref{fig:matdec}. We use
the estimated \(\mathbf{\hat{A}}\) matrix as the input to our
classification models.

The only difference between the topic modeling approach and the word
embedding approach is the structure of the original data matrix
\(\mathbf{X}\) obtained from the chosen NLP featurization method. In one
case, we obtain vector representations for documents; in the other, we
obtain vector representations for terms. For topic modeling, we start
with a documents by terms matrix and end up with ICA features which are
linear combinations of SVD features which are in turn linear
combinations of terms (hence, topics). When we start with the
word-context co-occurrence matrix, we obtain word embeddings, which are
vectors of ICA features which are linear combinations of SVD features
which are in turn linear combinations of context terms (again, we can
think of these as topics).

The number of components is also up for question - it seems that people
use values anywhere from 50 to 1,000.

\begin{figure}
  \centering
  \matbox{5}{7}{n}{m}{X} = 
  \matbox{5}{3}{n}{k}{U} \raiserows{1}{\matbox{3}{3}{k}{k}{\Sigma}}
  \raiserows{1}{\matbox{3}{7}{k}{m}{V^T}}
  \newline
  \newline
  \newline
  \matbox{5}{3}{n}{k}{U} = 
  \matbox{5}{3}{n}{p}{A} \raiserows{1}{\matbox{3}{3}{p}{k}{S}}
  \caption{Truncated Singular Value Decomposition followed by Independent Componenet Analysis.}
  \label{fig:matdec}
\end{figure}

\hypertarget{classification}{%
\subsection{Classification}\label{classification}}

\begin{itemize}
\tightlist
\item
  One-class SVM
\item
  Binary SVM
\end{itemize}

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

\begin{itemize}
\tightlist
\item
  Performance
\item
  Explainability

  \begin{itemize}
  \tightlist
  \item
    LIME (local)
  \item
    ICA

    \begin{itemize}
    \tightlist
    \item
      global
    \item
      local
    \end{itemize}
  \item
    Metric I came up with
  \end{itemize}
\end{itemize}

Penalty: \[
\frac{1}{N} \sum_{i=1}^{N} \frac{1}{T_i} \sum_{j=1}^{T_i} \mathbbm{1}_A(w_j) - \mathbbm{1}_B(w_j)
\] where \(A\) is the set of words that the classifier associated with
the correct class (e.g., tweet \(i\) is labeled as ``reliable,'' and the
classifier classified the tweet as ``reliable'') according to the LIME
output for tweet \(i\), \(B\) is the set of words that the classifier
associated with the wrong class according to the LIME output for tweet
\(i\), there are \(T_i\) words in tweet \(i\), and there are \(N\)
tweets.

No Penalty: \[
\frac{1}{N} \sum_{i=1}^{N} \frac{1}{T_i} \sum_{j=1}^{T_i} \mathbbm{1}_A(w_j)
\] where \(A\) is the set of words that the classifier associated with
the correct class according to the LIME output for tweet \(i\), there
are \(T_i\) words in tweet \(i\), and there are \(N\) tweets.

\hypertarget{results}{%
\section{Results}\label{results}}

\label{sec:results} Report evaluation metrics for\ldots{}

\begin{itemize}
\tightlist
\item
  One-class SVM
\item
  Binary SVM
\end{itemize}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\label{sec:discuss}

\begin{itemize}
\tightlist
\item
  Interpret results
\item
  Future work

  \begin{itemize}
  \tightlist
  \item
    Different ICA algorithm (not FastICA)
  \item
    Multiple ICA runs + take most representative of those
  \item
    Better ICA explainability tool?
  \item
    Better explainability metric?
  \item
    Other classifiers (e.g., neural nets)
  \item
    Incorporate other features:

    \begin{itemize}
    \tightlist
    \item
      Part-of-speech tag counts
    \item
      Punctuation counts
    \item
      Use of all-caps
    \item
      Sentiment analysis
    \end{itemize}
  \end{itemize}
\end{itemize}

\paragraph{Paragraph}

Misc. text.

\hypertarget{examples-of-citations-figures-tables-references}{%
\section{Examples of citations, figures, tables,
references}\label{examples-of-citations-figures-tables-references}}

\label{sec:others}

some text (Kour and Saabne 2014b, 2014a) and see Hadash et al. (2018).

The documentation for \verb+natbib+ may be found at

\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text. For example,

\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}

produces

\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}

\hypertarget{figures}{%
\subsection{Figures}\label{figures}}

Misc. text. See Figure \ref{fig:fig1}. Here is how you add footnotes.
{[}\^{}Sample of the first footnote.{]}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\hypertarget{tables}{%
\subsection{Tables}\label{tables}}

Misc. text.

See awesome Table\textasciitilde{}\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-hadash2018estimate}{}%
Hadash, Guy, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and
Alon Jacovi. 2018. ``Estimate and Replace: A Novel Approach to
Integrating Deep Neural Networks with Existing Applications.''
\emph{arXiv Preprint arXiv:1804.09028}.

\leavevmode\hypertarget{ref-kour2014fast}{}%
Kour, George, and Raid Saabne. 2014a. ``Fast Classification of
Handwritten on-Line Arabic Characters.'' In \emph{Soft Computing and
Pattern Recognition (Socpar), 2014 6th International Conference of},
312--18. IEEE.

\leavevmode\hypertarget{ref-kour2014real}{}%
---------. 2014b. ``Real-Time Segmentation of on-Line Handwritten Arabic
Script.'' In \emph{Frontiers in Handwriting Recognition (Icfhr), 2014
14th International Conference on}, 417--22. IEEE.

\bibliographystyle{unsrt}
\bibliography{references.bib}


\end{document}
