---
title: Automated Detection of Misinformation in Tweets about COVID-19
authors:
  - name: Caitlin Moroney
    thanks: Use footnote for providing further information about author (webpage, alternative address)---*not* for acknowledging funding agencies. Optional.
    department: Department of Mathematics and Statistics
    affiliation: American University
    location: Washington, DC 20016
    email: cm0246b@american.edu
abstract: |
  Enter the text of your abstract here.
keywords:
  - COVID-19
  - coronavirus
  - NLP
  - machine learning
  - misinformation
bibliography: references.bib
output: 
  rticles::arxiv_article:
    keep_tex: true
    includes:
      in_header: mystyles.sty
---

# Introduction

Insert introduction text here.

The rest of the paper is organized as follows: Section \ref{sec:methods} discusses our methodology; Section \ref{sec:results} presents the results of our experiments; Section \ref{sec:discuss} discusses our results and plans for future work.

# Methodology
\label{sec:methods}
This paper explores a series of methods which seek to exploit linguistic features in raw text data in order to perform the automated detection of unreliable tweets. The experiments follow the same general framework with certain implementation details tweaked for each experiment. The first step in this framework is the application of NLP featurization methods to the raw tweet text. While different featurization methods are compared, all methods involve the use of NLP tools to represent the text with numeric features. Subsequently, latent variable methods are employed to reduce the dimensionality of the resulting $\mathbf{X}$ feature matrix (as well as to uncover latent variables). The latent variables are then used in the classification task. Finally, we evaluate the classification algorithms paired with the featurization methods with respect to performance and explainability. We use the typical performance metrics, including accuracy, F-score, precision, recall, and ROC-AUC. We use LIME to evaluate local explainability for non-interpretable methods. Furthermore, we present a new explainability framework for latent variable methods as well as a new explainability metric. Below, we explore in greater detail the methods used for featurization, latent variables, classification, and evaluation of explainability.

## NLP featurization
In order to obtain features from the raw tweet text, we first employed standard preprocessing, to include removing stop words and punctuation as well as lemmatizing words. The two approaches we pursued involved (1) Bag-of-Words and (2) word embeddings, each followed by latent variable methods.

### Bag-of-Words
- Bag-of-Words + topic modeling
    - Vectorization: raw counts, tf-idf, binary
    - PCA + ICA

After constructing the Bag-of-Words matrix, we employed topic modeling to reduce the dimensionality of the $Nxp$ matrix.

### Word embeddings

To create word embeddings, we used the word-context co-occurrence matrix which, unlike the Bag-of-Words method, is able to incorporate information about context from the raw text data. We trained the embeddings on the full set of 560 tweets. With the Bag-of-Words methods, we are able to encode information about words' presence in a given document; the co-occurrence approach improves upon this by allowing us to look at word usage with respect to the presence of other words. We constructed a word-context co-occurrence matrix using the entire vocabulary for both target terms and context terms. In other words, the matrix was symmetric. We incorporated a number of hyperparameters related to the construction and subsequent transformation of this matrix, including context window size, the use of raw counts or variations on the Pointwise Mutual Information (PMI), and Laplace smoothing. We also included “\<START\>” and “\<END\>” tokens at the beginning and end of each tweet.

- Context window size

Window size refers to the number of tokens before and after the target word are scanned for context words. According to some sources, a window size of four or larger captures semantic representations of the words, whereas smaller windows capture more syntactic representations.

- Raw counts vs PMI (or PPMI)
- Shifted vs unshifted PMI (or PPMI)
- Laplace smoothing
- Use of start & end tokens

Latent variable methods were subsequently applied to the word embeddings in order to reduce the dimensionality.

Finally, we averaged over the word embeddings for the words in each tweet to obtain a single vector representation for each tweet.

Overall, it seemed that larger context windows prevailed - a window size of 15 +/- performed the best (I tried window sizes of 1, 2, 4, 6, 10, 15, and 20). I find this interesting because tweets are such short posts (perhaps 30 words encompasses the entirety of the tweet for most tweets). Incorporating add-one Laplace smoothing and shifted [P]PMI decreased performance across all metrics (accuracy, precision, recall, ROC AUC, and F1 score). Interestingly, PMI often outperformed not only raw frequencies but also PPMI (positive PMI). However, with other optimal hyperparameters held constant, there was virtually no difference in performance between PMI and PPMI. Optimal text cleaning included removing special characters which were not punctuation (parentheses, question marks, exclamation points, periods, commas, hyphens, colons, and semicolons) or alphanumeric characters (letters or numbers), converting all text to lowercase, removing stop words, and lemmatizing words (using NLTK’s WordNetLemmatizer aided by NLTK’s part-of-speech tagger).

**The best results included ROC AUC of 0.94, accuracy of 0.89, F1 score of 0.90, precision of 0.86, and recall of 0.94. These scores are all higher than the best results from last week and were obtained using the same nested CV procedure (3 folds for inner loop; 5 folds for outer loop).** NOTE: These should be replaced with the correct numbers.

- BERT embeddings from pre-trained model

## Latent variable methods
In order to reduce the dimensionality of the feature matrix $\mathbf{X}$, we employed latent variable methods. Specifically, we applied Principal Component Analysis (PCA) followed by Independent Component Analysis (ICA) to both the Bag-of-Words matrix and the word-context co-occurrence matrix.

In order to perform PCA, we rescale the data matrix and then apply Singular Value Decomposition (SVD). We first scale the data so that the columns have zero mean and unit variance. The SVD model is as follows:
$$\mathbf{X = U \Sigma V}^T$$
where $\mathbf{\Sigma}$ is a diagonal matrix containing the singular values of $\mathbf{X}$, $\mathbf{U}$ is BLAH, and $\mathbf{V}^T$ is BLAHBLAH.

In essence, we use SVD and keep only the rows of $\mathbf{U}$ and $\mathbf{V}^T$ that correspond to the largest $k$ singular values in the diagonal matrix $\mathbf{\Sigma}$, where $k$ is the desired order for the approximation of our initial data matrix. Therefore, if $k$ is less than ??? we have achieved dimensionality reduction.

- Explain ICA

The ICA model can be represented in matrix form as follows:
$$\mathbf{X = AS}$$

The pipeline involves performing SVD on the initial data matrix, $\mathbf{X}$, such that we can obtain the $\mathbf{U}$ matrix which contains the SVD feature vectors. This matrix is used as the input for ICA, so that we have $\mathbf{U = AS}$. Then, $\mathbf{S}$ is the whitened mixing matrix, and $\mathbf{A}$ contains the ICA features. This process is represented visually in figure XXXX.

The only difference between the topic modeling approach and the word embedding approach is the structure of the original data matrix $\mathbf{X}$ obtained from the chosen NLP featurization method. For topic modeling, we start with a documents by terms matrix and end up with ICA features which are linear combinations of SVD features which are in turn linear combinations of terms (hence, topics). When we start with the word-context co-occurrence matrix, we obtain word embeddings, which are vectors of ICA features which are linear combinations of SVD features which are in turn linear combinations of context terms (again, we can think of these as topics).

The number of components is also up for question - it seems that people use values anywhere from 50 to 1,000.

## Classification
- One-class SVM
- Binary SVM

## Evaluation
- Performance
- Explainability
    - LIME (local)
    - ICA
        - global
        - local
    - Metric I came up with

Penalty:
$$
\frac{1}{N} \sum_{i=1}^{N} \frac{1}{T_i} \sum_{j=1}^{T_i} 1_A(w_j) - 1_B(w_j)
$$
where $A$ is the set of words that the classifier associated with the correct class (e.g., tweet $i$ is labeled as “reliable,” and the classifier classified the tweet as “reliable”) according to the LIME output for tweet $i$, $B$ is the set of words that the classifier associated with the wrong class according to the LIME output for tweet $i$, there are $T_i$ words in tweet $i$, and there are $N$ tweets.

No Penalty:
$$
\frac{1}{N} \sum_{i=1}^{N} \frac{1}{T_i} \sum_{j=1}^{T_i} 1_A(w_j)
$$
where $A$ is the set of words that the classifier associated with the correct class according to the LIME output for tweet $i$, there are $T_i$ words in tweet $i$, and there are $N$ tweets.

# Results
\label{sec:results}
Report evaluation metrics for...

- One-class SVM
- Binary SVM

# Discussion
\label{sec:discuss}

- Interpret results
- Future work
    - Different ICA algorithm (not FastICA)
    - Multiple ICA runs + take most representative of those
    - Better ICA explainability tool?
    - Better explainability metric?
    - Other classifiers (e.g., neural nets)
    - Incorporate other features:
        - Part-of-speech tag counts
        - Punctuation counts
        - Use of all-caps
        - Sentiment analysis

\paragraph{Paragraph}
Misc. text.

# Examples of citations, figures, tables, references
\label{sec:others}

some text [@kour2014real; @kour2014fast] and see @hadash2018estimate.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,

\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}

produces

\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}

## Figures

Misc. text. 
See Figure \ref{fig:fig1}. Here is how you add footnotes. [^Sample of the first footnote.]

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

## Tables

Misc. text.

See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

# References

