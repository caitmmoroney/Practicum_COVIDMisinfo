---
title: Automated Detection of Misinformation in Tweets about COVID-19
authors:
  - name: Caitlin Moroney
    thanks: Use footnote for providing further information about author (webpage, alternative address)---*not* for acknowledging funding agencies. Optional.
    department: Department of Mathematics and Statistics
    affiliation: American University
    location: Washington, DC 20016
    email: cm0246b@american.edu
abstract: |
  Enter the text of your abstract here.
keywords:
  - COVID-19
  - coronavirus
  - NLP
  - machine learning
  - misinformation
bibliography: references.bib
output: 
  rticles::arxiv_article:
    keep_tex: true
---

# Introduction

Insert introduction text here.

The rest of the paper is organized as follows: Section \ref{sec:methods} discusses our methodology; Section \ref{sec:results} presents the results of our experiments; Section \ref{sec:discuss} discusses our results and plans for future work.

# Methodology
\label{sec:methods}
This paper explores a series of methods which seek to exploit linguistic features in raw text data in order to perform the automated detection of unreliable tweets. The experiments follow the same general framework with certain implementation details tweaked for each experiment. The first step in this framework is the application of NLP featurization methods to the raw tweet text. While different featurization methods are compared, all methods involve the use of NLP tools to represent the text with numeric features. Subsequently, latent variable methods are employed to reduce the dimensionality of the resulting $\mathbf{X}$ feature matrix (as well as to uncover latent variables). The latent variables are then used in the classification task. Finally, we evaluate the classification algorithms paired with the featurization methods with respect to performance and explainability. We use the typical performance metrics, including accuracy, F-score, precision, recall, and ROC-AUC. We use LIME [@ribeiro_why_2016] to evaluate local explainability for non-interpretable methods. Furthermore, we present a new explainability framework for latent variable methods as well as a new explainability metric. Below, we explore in greater detail the methods used for featurization, latent variables, classification, and evaluation of explainability.

## NLP featurization
In order to obtain features from the raw tweet text, we first employed standard preprocessing, to include removing stop words and punctuation as well as lemmatizing words. The two approaches we pursued involved (1) Bag-of-Words and (2) word embeddings, each followed by latent variable methods.

### Bag-of-Words
- Bag-of-Words + topic modeling
    - Vectorization: raw counts, tf-idf, binary
    - PCA + ICA

After constructing the Bag-of-Words matrix, we employed topic modeling to reduce the dimensionality of the $Nxp$ matrix.

### Word embeddings

To create word embeddings, we used the word-context co-occurrence matrix which, unlike the Bag-of-Words method, is able to incorporate information about context from the raw text data. We trained the embeddings on the full set of 560 tweets. With the Bag-of-Words methods, we are able to encode information about words' presence in a given document; the co-occurrence approach improves upon this by allowing us to look at word usage with respect to the presence of other words. We constructed a word-context co-occurrence matrix using the entire vocabulary for both target terms and context terms. In other words, the matrix was symmetric. We incorporated a number of hyperparameters related to the construction and subsequent transformation of this matrix, including context window size, the use of raw counts or variations on the Pointwise Mutual Information (PMI), and Laplace smoothing. We also included “\<START\>” and “\<END\>” tokens at the beginning and end of each tweet.

- Context window size

Window size refers to the number of tokens before and after the target word are scanned for context words. According to some sources, a window size of four or larger captures semantic representations of the words, whereas smaller windows capture more syntactic representations.

- Raw counts vs PMI (or PPMI)
- Shifted vs unshifted PMI (or PPMI)
- Laplace smoothing
- Use of start & end tokens

Latent variable methods were subsequently applied to the word embeddings in order to reduce the dimensionality.

Finally, we averaged over the word embeddings for the words in each tweet to obtain a single vector representation for each tweet.

Overall, it seemed that larger context windows prevailed - a window size of 15 +/- performed the best (I tried window sizes of 1, 2, 4, 6, 10, 15, and 20). I find this interesting because tweets are such short posts (perhaps 30 words encompasses the entirety of the tweet for most tweets). Incorporating add-one Laplace smoothing and shifted [P]PMI decreased performance across all metrics (accuracy, precision, recall, ROC AUC, and F1 score). Interestingly, PMI often outperformed not only raw frequencies but also PPMI (positive PMI). However, with other optimal hyperparameters held constant, there was virtually no difference in performance between PMI and PPMI. Optimal text cleaning included removing special characters which were not punctuation (parentheses, question marks, exclamation points, periods, commas, hyphens, colons, and semicolons) or alphanumeric characters (letters or numbers), converting all text to lowercase, removing stop words, and lemmatizing words (using NLTK’s WordNetLemmatizer aided by NLTK’s part-of-speech tagger).

**The best results included ROC AUC of 0.94, accuracy of 0.89, F1 score of 0.90, precision of 0.86, and recall of 0.94. These scores are all higher than the best results from last week and were obtained using the same nested CV procedure (3 folds for inner loop; 5 folds for outer loop).** NOTE: These should be replaced with the correct numbers.

- BERT embeddings from pre-trained model

## Latent variable methods
In order to reduce the dimensionality of the data matrix $\mathbf{X}$, we employed latent variable methods. Specifically, we followed the methodology presented in @honkela_wordicaemergence_2010: we applied Principal Component Analysis (PCA) followed by Independent Component Analysis (ICA) to both the Bag-of-Words matrix and the word-context co-occurrence matrix.

In order to perform PCA, we rescale the data matrix and then apply Singular Value Decomposition (SVD). We first scale the data so that the columns have zero mean and unit variance. The SVD model is as follows:
$$\mathbf{X = U \Sigma V}^T$$
where $\mathbf{\Sigma}$ is a diagonal matrix containing the singular values of $\mathbf{X}$, $\mathbf{U}$ is BLAH, and $\mathbf{V}^T$ is BLAHBLAH.

To achieve PCA, we can use truncated SVD. In essence, we perform SVD and keep only the columns of $\mathbf{U}$ and $\mathbf{V}^T$ that correspond to the largest $k$ singular values in the diagonal matrix $\mathbf{\Sigma}$, where $k$ is the desired order for the approximation of our initial data matrix. Therefore, if $k$ is less than $m$, we have achieved dimensionality reduction.

- Explain ICA

The ICA model can be represented in matrix form as follows:
$$\mathbf{X = AS}$$

The pipeline involves performing SVD on the initial data matrix, $\mathbf{X}$, such that we can obtain the $\mathbf{U}$ matrix which contains the SVD feature vectors. This matrix is used as the input for ICA, so that we have $\mathbf{U = AS}$. Then, $\mathbf{S}$ is the whitened mixing matrix, and $\mathbf{A}$ contains the ICA features. This process is represented visually in Figure \ref{fig:matdec}. We use the estimated $\mathbf{\hat{A}}$ matrix as the input to our classification models.

The only difference between the topic modeling approach and the word embedding approach is the structure of the original data matrix $\mathbf{X}$ obtained from the chosen NLP featurization method. In one case, we obtain vector representations for documents; in the other, we obtain vector representations for terms. For topic modeling, we start with a documents by terms matrix and end up with ICA features which are linear combinations of SVD features which are in turn linear combinations of terms (hence, topics). When we start with the word-context co-occurrence matrix, we obtain word embeddings, which are vectors of ICA features which are linear combinations of SVD features which are in turn linear combinations of context terms (again, we can think of these as topics).

The number of components is also up for question - it seems that people use values anywhere from 50 to 1,000.

\begin{figure}
  \centering
  \matbox{5}{7}{n}{m}{X} = 
  \matbox{5}{3}{n}{k}{U} \raiserows{1}{\matbox{3}{3}{k}{k}{\Sigma}}
  \raiserows{1}{\matbox{3}{7}{k}{m}{V^T}}
  \newline
  \newline
  \newline
  \matbox{5}{3}{n}{k}{U} = 
  \matbox{5}{3}{n}{p}{A} \raiserows{1}{\matbox{3}{3}{p}{k}{S}}
  \caption{Truncated Singular Value Decomposition followed by Independent Componenet Analysis.}
  \label{fig:matdec}
\end{figure}

## Classification
- One-class SVM
- Binary SVM

## Evaluation
In order to evaluate our experiments, including featurization methods and classification algorithms, we have identified three areas for comparison: performance, computational complexity, and explainability. To measure performance, we employ the standard suite of evaluation metrics, i.e., accuracy, F-score, precision, recall, and ROC-AUC. We report the macro-averaged versions of these scores. Computational complexity is easily captured by the amount of time taken for training and testing. Evaluating the explainability of one method versus another proves to be a more complicated task. For the experiments using word embeddings obtained from the pretrained BERT model, we use LIME [@ribeiro_why_2016] to obtain local explanations for tweet predictions. For the ICA word embeddings, we propose a new framework to obtain global and local explanations for tweet predictions which derive from the ICA matrix decomposition. In order to then compare overall explainability for one method versus another, we have devised a metric which captures information from the LIME and ICA local explanations and aggregates these values to produce a single number representing an explainability score. This allows us to compare two methods with respect to explainability in the same way that we might compare them in terms of accuracy or precision.

Our goal with assessing explainability is to determine whether the machine learning pipeline is making what we would consider intuitive decisions. In other words, when a human and the machine look at the same tweet, we are not only interested in knowing whether the machine can make the same classification as the human (which we can measure with accuracy), but we are also interested in knowing whether the machine and the human make the same judgment for similar reasons.

### LIME
Explain LIME.

### ICA
Explain ICA explainability.

#### Global

#### Local

### Explainability metric
As mentioned above, our initial motivation in pursuing explainability was to provide an evaluation metric which would allow us to compare the overall explainability of one model and featurization method with that of another. The final result of this effort is a novel metric which allows us to incorporate the LIME [@ribeiro_why_2016] and ICA explainability output into a single score which represents the overall explainability as an aggregation of local explanations.

Because our conception of explainability inherently relies on a comparison to human decision-making, our explainability metric takes an input which captures the rules our coders used to label our set of tweets. This input is in the form of a vocabulary list which comprises words that fall under the rules in Table 1 from @boukouvalas_independent_2020.

Penalty:
$$
\frac{1}{N} \sum_{i=1}^{N} \frac{1}{T_i} \sum_{j=1}^{T_i} \mathbbm{1}_A(w_j) - \mathbbm{1}_B(w_j)
$$
where $A$ is the set of words that the classifier associated with the correct class (e.g., tweet $i$ is labeled as “reliable,” and the classifier classified the tweet as “reliable”) according to the LIME output for tweet $i$, $B$ is the set of words that the classifier associated with the wrong class according to the LIME output for tweet $i$, there are $T_i$ words in tweet $i$, and there are $N$ tweets.

No Penalty:
$$
\frac{1}{N} \sum_{i=1}^{N} \frac{1}{T_i} \sum_{j=1}^{T_i} \mathbbm{1}_A(w_j)
$$
where $A$ is the set of words that the classifier associated with the correct class according to the LIME output for tweet $i$, there are $T_i$ words in tweet $i$, and there are $N$ tweets.

# Results
\label{sec:results}
Report evaluation metrics for...

- One-class SVM
- Binary SVM

# Discussion
\label{sec:discuss}

- Interpret results
- Future work
    - Different ICA algorithm (not FastICA)
    - Multiple ICA runs + take most representative of those
    - Better ICA explainability tool?
    - Better explainability metric?
    - Other classifiers (e.g., neural nets)
    - Incorporate other features:
        - Part-of-speech tag counts
        - Punctuation counts
        - Use of all-caps
        - Sentiment analysis

\paragraph{Paragraph}
Misc. text.

# Examples of citations, figures, tables, references
\label{sec:others}

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,

\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}

produces

\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}

## Tables

Misc. text.

See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

# References

